# FKS Trading Platform - AI Coding Agent Instructions

## ðŸŽ¯ Quick Reference

| Category | Command / URL | Description |
|----------|---------------|-------------|
| **Start Services** | `make up` | Standard 8 services |
| | `make gpu-up` | With Ollama LLM + fks_ai (Phase 6) |
| **Phase 6 AI** | `docker-compose exec fks_ai pytest tests/unit/ -v` | AI unit tests (70 tests) |
| | `docker-compose exec fks_ai pytest tests/integration/ -v` | AI integration (18 tests) |
| | `curl http://localhost:8006/docs` | FastAPI Swagger UI |
| **Testing** | `docker-compose exec fks_app pytest tests/unit/strategies/asmbtr/` | ASMBTR suite (108/108) |
| | `docker-compose exec fks_app pytest tests/unit/validators/` | Quality validators (34/34) |
| | `docker-compose exec fks_app pytest tests/unit/cache/` | Redis caching (20/20) |
| **Code Quality** | `make lint` | Run ruff + mypy + black |
| | `make format` | Auto-format with black + isort |
| **Monitoring** | http://localhost:8000/health/dashboard/ | System health overview |
| | http://localhost:3000 | Grafana dashboards |
| | http://localhost:9090 | Prometheus metrics |
| **Database** | `make migrate` | Run Django migrations |
| | `make db-shell` | PostgreSQL psql shell |

**Current Status:** âœ… Phase 7.1 COMPLETE - Evaluation Framework DEPLOYED (Oct 31, 2025)  
**Next Phase:** ðŸŽ¯ Phase 7.2 - LLM-Judge Audits  
**Architecture:** 8-Service Microservices | **Stack:** Python 3.13, FastAPI, Django, PostgreSQL, TimescaleDB, Redis  
**Test Status:** 282/282 passing (ASMBTR: 108, Redis: 20, Validators: 34, Monitoring: 40, AI: 88, Evaluation: 6 integration)

## ðŸ“‹ Project Overview

**FKS Main** is the **orchestrator and monitoring hub** for an **8-service microservices architecture** providing algorithmic trading, AI-powered strategy generation, and comprehensive market analysis.

### System Architecture (Monorepo Multi-Container)

FKS uses a **monorepo architecture** with Docker containers under `src/services/` for each microservice. All code lives in a single Git repository for simplified development while maintaining service isolation.

| Service | Port | Responsibilities | Status |
|---------|------|------------------|--------|
| **fks_main** | 8000 | Orchestrator, service registry, health monitoring, Celery Beat | âœ… |
| **fks_api** | 8001 | Gateway with routing, JWT auth, rate limiting | âœ… |
| **fks_app** | 8002 | Business logic: strategies, signals, portfolio optimization | âœ… |
| **fks_data** | 8003 | Market data collection (CCXT), TimescaleDB storage | âœ… |
| **fks_execution** | 8004 | Rust execution engine (ONLY talks to exchanges) | â¸ï¸ Runtime issue |
| **fks_ninja** | 8005 | C# .NET bridge to NinjaTrader 8 for prop firms | âœ… |
| **fks_ai** | 8006 | Multi-agent AI: LangGraph, Ollama, ChromaDB, 7 agents | âœ… Phase 6 complete |
| **fks_web** | 3001 | Django/Vite web UI with Bootstrap 5 | â¸ï¸ Architecture review |

### Current Status: Phase 7.1 COMPLETE âœ… (Oct 31, 2025)

**Latest Achievement**: Evaluation framework with confusion matrices and statistical testing
- âœ… **Phase 6.1-6.5**: Multi-agent AI system DEPLOYED (LangGraph + Ollama + ChromaDB)
- âœ… **Phase 7.1**: Evaluation Framework - ModelEvaluator, Bonferroni/BH corrections, chi-square testing
- âœ… **Phase 6.3**: Graph Orchestration - StateGraph with conditional routing
- âœ… **Phase 6.4**: Testing & API - 88 tests (70 unit + 18 integration), 4 FastAPI endpoints
- âœ… **Phase 6.5**: Deployment - Docker build, Ollama llama3.2:3b (2GB), services operational

**Deployment Status**:
- âœ… fks_ai container built with Python 3.13 (fixed import structure)
- âœ… Ollama llama3.2:3b model downloaded and ready (2.0 GB)
- âœ… Services running: fks_ai (8006), ollama (11434)
- âœ… Health endpoint responding: `curl http://localhost:8006/health`
- âœ… Memory system operational (ChromaDB)

**AI Capabilities**:
- **7 Specialized Agents**: Technical, Sentiment, Macro, Risk analysts + Bull/Bear/Manager debaters
- **StateGraph Pipeline**: Analysts â†’ Debate â†’ Manager â†’ Signal â†’ Reflection
- **ChromaDB Memory**: Persistent decision storage with semantic search
- **Risk Management**: Position sizing, stop-loss, take-profit calculation
- **REST API**: 4 endpoints (analyze, debate, memory, status) with OpenAPI docs

**Infrastructure**:
- 16/16 services operational (100% health with fks_ai)
- Database: PostgreSQL + TimescaleDB + pgvector + Redis
- AI: Ollama llama3.2:3b (local LLM), ChromaDB (memory), sentence-transformers
- Monitoring: Prometheus + Grafana + Alertmanager
- Tests: 276/276 passing (188 Phase 5 + 88 Phase 6)

**Next Steps**: Phase 7 - Evaluation & Advanced Models
- Implement confusion matrices for ASMBTR/ML models
- LLM-judge audits for factual consistency
- Walk-forward optimization (WFO)
- CPI-Gold hedging strategy

**Important**: When working with services, note that:
- Code is in `src/services/[service_name]/src/` (e.g., `src/services/api/src/main.py`)
- Each service has its own README.md with detailed architecture
- Services communicate via HTTP APIs within docker-compose network
- Django settings: `src/services/web/src/django/settings.py`
- Root `manage.py` sets `DJANGO_SETTINGS_MODULE=services.web.src.django.settings`

## ðŸ—ï¸ Architecture Essentials

### Data Flow Diagram

```
Market Data: Exchanges â†’ fks_data (collect) â†’ TimescaleDB/Redis â†’ fks_app (query)
Signal Flow: fks_app (signal) â†’ fks_execution (order) â†’ Exchange
AI/ML Flow: fks_app (request) â†’ fks_ai (GPU inference/RAG) â†’ fks_app (prediction)
API Flow:   Client â†’ fks_api (auth) â†’ fks_app (logic) â†’ fks_api (response)
NinjaTrader: fks_app (signal) â†’ fks_ninja (bridge) â†’ NT8 â†’ Prop Firm
```### Service Responsibilities

| Service | Responsibilities | Critical Rules |
|---------|------------------|----------------|
| **fks_main** | Orchestrator, registry, health monitoring | NO business logic, NO exchange communication |
| **fks_api** | Gateway, JWT auth, rate limiting | Pure routing - NO domain logic |
| **fks_app** | Strategies, signals, portfolio optimization | ALL trading intelligence here |
| **fks_ai** | Local LLM (Ollama), regime detection, RAG | Zero-cost AI inference |
| **fks_data** | Market data collection, TimescaleDB storage | Other services query here, NEVER exchanges directly |
| **fks_execution** | Rust order execution | ONLY service that talks to exchanges |
| **fks_ninja** | C# NinjaTrader 8 bridge | Forward signals to prop firm platforms |
| **fks_web** | Django UI with Bootstrap 5 | Fetch ALL data via fks_api |

---

## ðŸ¦ fks_ninja: Bitcoin-First Prop Firm Integration

### Overview: Progressive Wealth Growth Strategy

**fks_ninja** is the C# bridge to NinjaTrader 8 (NT8) that enables **automated prop firm trading** with a **Bitcoin-first philosophy**. The service implements a **progressive scaling model**: Start with a hardware wallet tracking your BTC holdings, grow through spending wallets (Shakepay, Crypto.com, Netcoins), scale to multiple prop firm accounts, and execute synchronized trades across all active accounts when FKS signals fire.

**Core Philosophy**:
- **Bitcoin as Base Currency**: All wealth tracking and recommendations use BTC as the foundation
- **Hardware Wallet First**: Users must set up a hardware wallet (Ledger, Trezor) to track public keys in fks_app
- **Progressive Scaling**: System adapts recommendations based on total capital (hardware + spending wallets + prop accounts)
- **Multi-Account Execution**: Single FKS signal â†’ automated execution across all eligible prop accounts
- **Crypto-Native**: Prioritize crypto prop firms (HyroTrader, Crypto Funded Trader) with direct API access

### Architecture: Three-Layer Integration

```
Layer 1: Wallet Tracking (fks_app)
  â”œâ”€ Hardware Wallet (BTC pub key monitoring)
  â”œâ”€ Spending Wallets (Shakepay/Crypto.com/Netcoins APIs)
  â””â”€ Total Capital Calculation â†’ Dynamic Recommendations

Layer 2: Signal Distribution (fks_ninja)
  â”œâ”€ FKS Signal Reception (socket/file-based)
  â”œâ”€ Account Filtering (asset availability check)
  â””â”€ Multi-Account Routing

Layer 3: Execution
  â”œâ”€ Traditional: NT8 â†’ Rithmic/Tradovate (Apex, Take Profit Trader, OneStep)
  â””â”€ Crypto: Direct API â†’ Bybit/Exchange (HyroTrader, Crypto Funded Trader)
```

### Wallet Integration (Layer 1)

#### Hardware Wallet Setup

**User Onboarding Flow**:
1. **Prompt User**: "To start tracking your wealth, please set up a hardware wallet (recommended: Ledger Nano X, Trezor Model T)"
2. **Obtain Public Key**: User provides BTC extended public key (xpub/zpub)
3. **Store in fks_app**: Save to `user_wallets` table in PostgreSQL
4. **Background Monitoring**: Celery task queries blockchain APIs (Blockstream, BlockCypher) every 15 minutes

**Implementation** (`src/services/app/src/wallets/hardware_wallet.py`):
```python
from blockstream import BlockstreamAPI
from typing import Dict, List

class HardwareWalletTracker:
    def __init__(self, xpub: str, wallet_type: str = 'ledger'):
        self.xpub = xpub
        self.api = BlockstreamAPI()
    
    async def get_balance(self) -> Dict[str, float]:
        """Query BTC balance from extended public key"""
        addresses = self.derive_addresses(self.xpub, count=20)  # First 20 addresses
        balances = await self.api.get_balances(addresses)
        
        total_btc = sum(balances.values())
        usd_price = await self.get_btc_price()  # From fks_data
        
        return {
            'btc': total_btc,
            'usd': total_btc * usd_price,
            'last_updated': datetime.now()
        }
    
    def derive_addresses(self, xpub: str, count: int) -> List[str]:
        """Derive Bitcoin addresses from xpub using BIP32"""
        # Use bitcoinlib or similar for BIP32 derivation
        pass
```

**Database Schema** (`src/services/app/src/models/wallet.py`):
```python
from django.db import models
from authentication.models import User

class UserWallet(models.Model):
    WALLET_TYPES = [
        ('hardware_btc', 'Hardware Wallet (BTC)'),
        ('shakepay', 'Shakepay'),
        ('crypto_com', 'Crypto.com'),
        ('netcoins', 'Netcoins'),
    ]
    
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    wallet_type = models.CharField(max_length=20, choices=WALLET_TYPES)
    public_key = models.CharField(max_length=255)  # xpub for hardware, API key ref for exchanges
    balance_btc = models.DecimalField(max_digits=18, decimal_places=8)
    balance_usd = models.DecimalField(max_digits=18, decimal_places=2)
    last_synced = models.DateTimeField(auto_now=True)
    
    class Meta:
        unique_together = ('user', 'wallet_type', 'public_key')
```

#### Spending Wallet Integration

**Supported Platforms**:
- **Shakepay**: Canadian exchange, BTC/ETH only, no official API (use browser automation or CSV imports)
- **Crypto.com**: REST API with OAuth2, supports 100+ assets
- **Netcoins**: Canadian fiat on-ramp, limited API (manual tracking initially)

**Implementation** (`src/services/app/src/wallets/spending_wallet.py`):
```python
import requests
from cryptography.fernet import Fernet

class CryptoComWallet:
    BASE_URL = 'https://api.crypto.com/v2/'
    
    def __init__(self, api_key: str, api_secret: str):
        self.api_key = api_key
        self.api_secret = api_secret
    
    async def get_balances(self) -> Dict[str, float]:
        """Fetch all asset balances"""
        headers = self._generate_auth_headers()
        response = requests.get(f"{self.BASE_URL}private/get-account-summary", headers=headers)
        
        data = response.json()['result']['accounts']
        return {
            account['currency']: float(account['balance'])
            for account in data
        }
    
    def _generate_auth_headers(self) -> Dict[str, str]:
        """Generate HMAC-SHA256 signature for API requests"""
        # Implement Crypto.com's authentication scheme
        pass
```

**Security Note**: Store API keys encrypted in PostgreSQL using `django-encrypted-model-fields`:
```python
from encrypted_model_fields.fields import EncryptedCharField

class WalletAPIKey(models.Model):
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    platform = models.CharField(max_length=20)
    api_key = EncryptedCharField(max_length=255)
    api_secret = EncryptedCharField(max_length=255)
```

#### Dynamic Recommendations Engine

**Capital Tiers & Recommendations**:

| Total Capital (BTC) | Total Capital (USD) | Recommended Action | Prop Firm Target |
|---------------------|---------------------|-------------------|------------------|
| 0.001 - 0.01 BTC | $50 - $500 | Focus on accumulation via spending wallets | None (too small) |
| 0.01 - 0.1 BTC | $500 - $5,000 | Open first crypto prop account (HyroTrader $5K) | 1 account |
| 0.1 - 0.5 BTC | $5,000 - $25,000 | Scale to 2-3 prop accounts (HyroTrader $25K + futures) | 2-3 accounts |
| 0.5 - 2 BTC | $25,000 - $100,000 | Diversify: 5+ prop accounts across crypto/futures | 5-8 accounts |
| 2+ BTC | $100,000+ | Max scaling: 10+ accounts, consider personal capital deployment | 10+ accounts |

**Implementation** (`src/services/app/src/recommendations/wealth_advisor.py`):
```python
class WealthAdvisor:
    def __init__(self, user_id: int):
        self.user = User.objects.get(id=user_id)
        self.total_btc = self._calculate_total_btc()
    
    def _calculate_total_btc(self) -> float:
        """Sum BTC across all wallets (hardware + spending)"""
        wallets = UserWallet.objects.filter(user=self.user)
        return sum([w.balance_btc for w in wallets])
    
    def get_recommendations(self) -> Dict:
        """Dynamic recommendations based on capital tier"""
        btc_price = self.get_btc_price()
        usd_total = self.total_btc * btc_price
        
        if usd_total < 500:
            return {
                'tier': 'accumulation',
                'action': 'Focus on DCA (dollar-cost averaging) via Shakepay/Netcoins',
                'prop_accounts': 0,
                'next_milestone': '$500 (0.01 BTC) to qualify for first prop account'
            }
        elif usd_total < 5000:
            return {
                'tier': 'starter',
                'action': 'Open HyroTrader $5K account (one-time $99 fee)',
                'prop_accounts': 1,
                'prop_details': [
                    {'name': 'HyroTrader', 'size': '$5,000', 'split': '90%', 'fee': '$99'}
                ],
                'next_milestone': '$5,000 (0.1 BTC) to scale to multiple accounts'
            }
        elif usd_total < 25000:
            return {
                'tier': 'scaling',
                'action': 'Add 2nd prop account: HyroTrader $25K or Apex futures',
                'prop_accounts': 2,
                'prop_details': [
                    {'name': 'HyroTrader', 'size': '$25,000', 'split': '90%', 'fee': '$199'},
                    {'name': 'Apex (futures)', 'size': '$25,000', 'split': '90%', 'fee': '$147'}
                ],
                'next_milestone': '$25,000 (0.5 BTC) to diversify across 5+ accounts'
            }
        # ... additional tiers
```

**User Interface** (`src/services/web/templates/dashboard/wealth_overview.html`):
```html
<div class="wealth-dashboard">
    <h2>Your Bitcoin Wealth</h2>
    
    <div class="total-holdings">
        <span class="btc-amount">{{ total_btc|floatformat:8 }} BTC</span>
        <span class="usd-amount">${{ total_usd|floatformat:2 }}</span>
    </div>
    
    <div class="wallet-breakdown">
        <h3>Wallet Breakdown</h3>
        {% for wallet in wallets %}
        <div class="wallet-row">
            <span>{{ wallet.wallet_type }}</span>
            <span>{{ wallet.balance_btc|floatformat:8 }} BTC</span>
        </div>
        {% endfor %}
    </div>
    
    <div class="recommendations">
        <h3>Recommended Next Steps</h3>
        <p>{{ recommendations.action }}</p>
        <p>Target Prop Accounts: {{ recommendations.prop_accounts }}</p>
    </div>
</div>
```

### Signal Distribution & Execution (Layers 2-3)

#### NT8 Strategy Listener (Traditional Prop Firms)

**Supported Firms**:
- **Apex Trader Funding**: Rithmic connection, trailing drawdown, 90% split
- **Take Profit Trader**: Tradovate connection, 100% profit split, no EOD rule
- **OneStep Funding**: Rithmic, 90% split, instant funding

**Implementation** (`src/services/ninja/src/Strategies/FKS_Listener.cs`):
```csharp
using System;
using System.Net.Sockets;
using System.Text.Json;
using NinjaTrader.NinjaScript.Strategies;

namespace NinjaTrader.NinjaScript.Strategies
{
    public class FKS_Listener : Strategy
    {
        private TcpClient _client;
        private const string FKS_HOST = "localhost";
        private const int FKS_PORT = 8007;  // fks_ninja signal port
        
        protected override void OnStateChange()
        {
            if (State == State.SetDefaults)
            {
                Description = "FKS Signal Listener for Prop Firm Trading";
                Name = "FKS_Listener";
                Calculate = Calculate.OnBarClose;
                EntriesPerDirection = 1;
                EntryHandling = EntryHandling.AllEntries;
            }
            else if (State == State.Configure)
            {
                _client = new TcpClient(FKS_HOST, FKS_PORT);
            }
        }
        
        protected override void OnBarUpdate()
        {
            // Check for new signals from FKS
            if (IsSignalAvailable())
            {
                var signal = ReceiveSignal();
                ExecuteSignal(signal);
            }
            
            // EOD timeout: Close all positions 15 mins before session end
            if (IsNearEOD())
            {
                ExitLong();
                ExitShort();
            }
        }
        
        private FKSSignal ReceiveSignal()
        {
            byte[] buffer = new byte[1024];
            int bytesRead = _client.GetStream().Read(buffer, 0, buffer.Length);
            string json = System.Text.Encoding.UTF8.GetString(buffer, 0, bytesRead);
            
            return JsonSerializer.Deserialize<FKSSignal>(json);
        }
        
        private void ExecuteSignal(FKSSignal signal)
        {
            if (signal.Action == "buy" && signal.Instrument == Instrument.FullName)
            {
                EnterLongLimit(0, true, 1, signal.Price, "FKS_Long");
                SetProfitTarget("FKS_Long", CalculationMode.Ticks, signal.TPPoints);
                SetStopLoss("FKS_Long", CalculationMode.Ticks, signal.SLPoints, false);
            }
            else if (signal.Action == "sell" && signal.Instrument == Instrument.FullName)
            {
                EnterShortLimit(0, true, 1, signal.Price, "FKS_Short");
                SetProfitTarget("FKS_Short", CalculationMode.Ticks, signal.TPPoints);
                SetStopLoss("FKS_Short", CalculationMode.Ticks, signal.SLPoints, false);
            }
        }
        
        private bool IsNearEOD()
        {
            // Check if within 15 minutes of session close
            var sessionEnd = SessionIterator.ActualSessionEnd;
            return (sessionEnd - Time[0]).TotalMinutes <= 15;
        }
    }
    
    public class FKSSignal
    {
        public string Action { get; set; }  // "buy" or "sell"
        public string Instrument { get; set; }  // "ES 03-25"
        public double Price { get; set; }
        public int TPPoints { get; set; }  // Take profit in ticks
        public int SLPoints { get; set; }  // Stop loss in ticks
    }
}
```

**Signal Server** (`src/services/ninja/src/signal_server.py`):
```python
import asyncio
import json
from typing import List

class SignalServer:
    """TCP server to send signals to NT8 instances"""
    
    def __init__(self, host='0.0.0.0', port=8007):
        self.host = host
        self.port = port
        self.clients: List[asyncio.StreamWriter] = []
    
    async def start(self):
        server = await asyncio.start_server(self.handle_client, self.host, self.port)
        async with server:
            await server.serve_forever()
    
    async def handle_client(self, reader, writer):
        """Register NT8 client"""
        self.clients.append(writer)
        print(f"NT8 client connected from {writer.get_extra_info('peername')}")
        
        # Keep connection alive
        try:
            while True:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            self.clients.remove(writer)
    
    async def broadcast_signal(self, signal: dict):
        """Send signal to all connected NT8 instances"""
        message = json.dumps(signal).encode('utf-8')
        
        for writer in self.clients:
            try:
                writer.write(message)
                await writer.drain()
            except Exception as e:
                print(f"Failed to send to client: {e}")
                self.clients.remove(writer)
```

#### Crypto Prop Firm Direct API (Preferred for Bitcoin Trading)

**Supported Platforms**:
- **HyroTrader**: Crypto-only, Bybit API, up to $200K funding, 90% split
- **Crypto Funded Trader**: Multi-exchange, API support, instant payouts

**Implementation** (`src/services/ninja/src/crypto_execution.py`):
```python
import ccxt
from typing import Dict, List

class HyroTraderExecutor:
    """Direct Bybit API execution for HyroTrader accounts"""
    
    def __init__(self, api_key: str, api_secret: str, testnet: bool = False):
        self.exchange = ccxt.bybit({
            'apiKey': api_key,
            'secret': api_secret,
            'enableRateLimit': True,
            'options': {'defaultType': 'swap'}  # Perpetual futures
        })
        
        if testnet:
            self.exchange.set_sandbox_mode(True)
    
    async def execute_signal(self, signal: Dict):
        """Execute FKS signal on Bybit"""
        symbol = signal['symbol']  # e.g., 'BTC/USDT:USDT'
        side = 'buy' if signal['action'] == 'long' else 'sell'
        amount = self.calculate_position_size(signal)
        
        # Place limit order
        order = await self.exchange.create_limit_order(
            symbol=symbol,
            side=side,
            amount=amount,
            price=signal['price']
        )
        
        # Set TP/SL using Bybit's bracket orders
        await self.set_tp_sl(order['id'], signal['tp_price'], signal['sl_price'])
        
        return order
    
    def calculate_position_size(self, signal: Dict) -> float:
        """Calculate position size based on account balance and risk %"""
        balance = self.exchange.fetch_balance()
        btc_balance = balance['BTC']['free']
        
        risk_per_trade = 0.02  # 2% risk per trade
        btc_risk = btc_balance * risk_per_trade
        
        # Calculate position size based on stop loss distance
        sl_distance = abs(signal['price'] - signal['sl_price'])
        position_size = btc_risk / sl_distance
        
        return position_size
    
    async def set_tp_sl(self, order_id: str, tp_price: float, sl_price: float):
        """Set take profit and stop loss for existing order"""
        await self.exchange.edit_order(
            id=order_id,
            params={
                'takeProfit': tp_price,
                'stopLoss': sl_price
            }
        )
```

#### Multi-Account Signal Router

**Implementation** (`src/services/ninja/src/account_manager.py`):
```python
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class PropAccount:
    id: int
    user_id: int
    firm_name: str  # 'hyrotrader', 'apex', 'takeprofit'
    account_type: str  # 'crypto' or 'futures'
    api_credentials: Dict
    active: bool
    supported_assets: List[str]

class MultiAccountRouter:
    """Routes FKS signals to all eligible prop accounts"""
    
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.accounts = self.load_accounts()
    
    def load_accounts(self) -> List[PropAccount]:
        """Load all active prop accounts for user"""
        return PropAccount.objects.filter(
            user_id=self.user_id,
            active=True
        ).all()
    
    async def route_signal(self, signal: Dict):
        """Send signal to all accounts that support the asset"""
        asset = signal['symbol'].split('/')[0]  # Extract BTC from BTC/USDT
        
        eligible_accounts = [
            acc for acc in self.accounts
            if asset in acc.supported_assets
        ]
        
        results = []
        for account in eligible_accounts:
            if account.account_type == 'crypto':
                executor = HyroTraderExecutor(**account.api_credentials)
                result = await executor.execute_signal(signal)
            else:  # futures via NT8
                result = await self.send_to_nt8(account, signal)
            
            results.append({
                'account': account.firm_name,
                'order': result
            })
        
        return results
    
    async def send_to_nt8(self, account: PropAccount, signal: Dict):
        """Send signal to NT8 via signal server"""
        server = SignalServer()  # Connect to running server
        await server.broadcast_signal({
            'action': signal['action'],
            'instrument': signal['nt8_instrument'],  # e.g., 'ES 03-25'
            'price': signal['price'],
            'tp_points': signal['tp_points'],
            'sl_points': signal['sl_points']
        })
```

### Progressive Scaling Example

**User Journey: Sarah's Bitcoin Growth**

**Month 1: Accumulation Phase**
- Sarah sets up Ledger Nano X, adds xpub to FKS
- Hardware wallet: 0.005 BTC ($250)
- Shakepay DCA: Buys $50/week BTC
- FKS recommendation: "Keep accumulating, need $500 to start prop trading"

**Month 3: First Prop Account**
- Total: 0.012 BTC ($600)
- FKS recommendation: "You qualify for HyroTrader $5K account!"
- Sarah pays $99 one-time fee, passes evaluation
- Now eligible for FKS automated trading on 1 account

**Month 6: Multi-Account Scaling**
- Total: 0.08 BTC ($4,000) across hardware + Shakepay + HyroTrader profits
- FKS recommendation: "Scale to 2 accounts: Add Apex futures or HyroTrader $25K"
- Opens Apex $25K account ($147 fee)
- Now trading 2 accounts simultaneously

**Month 12: Mature Portfolio**
- Total: 0.4 BTC ($20,000)
- Active accounts: 5 (2x HyroTrader crypto, 2x Apex futures, 1x Take Profit Trader)
- FKS executes: BTC long signal â†’ Fires on HyroTrader accounts only (asset match)
- ES short signal â†’ Fires on Apex + Take Profit Trader (futures match)

### Security & Compliance

**Hardware Wallet Best Practices**:
- **Never store private keys in FKS**: Only public keys (xpub/zpub)
- **User education**: Prompt to verify addresses on device before sharing
- **Backup reminders**: Celery task sends monthly reminder to verify seed phrase backup

**API Key Storage**:
- Encrypt at rest using Fernet (symmetric encryption)
- Decrypt only in memory during API calls
- Implement key rotation every 90 days

**Prop Firm Compliance**:
- **EOD Rules**: Always close positions before session end (Apex trailing drawdown)
- **Max Contracts**: Respect firm limits (e.g., 10 contracts for Apex $25K)
- **No Martingale**: FKS signals are independent, no position averaging
- **Audit Trail**: Log every signal, execution, and P&L to TimescaleDB

### Development Roadmap

**Phase 1: Wallet Tracking (2-3 weeks)**
- [ ] Implement hardware wallet xpub monitoring
- [ ] Integrate Crypto.com API for spending wallets
- [ ] Build wealth aggregation dashboard
- [ ] Add dynamic recommendation engine

**Phase 2: Single Account Execution (2-3 weeks)**
- [ ] Build NT8 signal server (TCP sockets)
- [ ] Develop FKS_Listener.cs strategy with TP/SL/EOD
- [ ] Test on Apex/Take Profit Trader simulations
- [ ] Implement HyroTrader direct API executor

**Phase 3: Multi-Account Scaling (3-4 weeks)**
- [ ] Create PropAccount model and manager
- [ ] Build multi-account signal router
- [ ] Implement asset filtering (crypto vs. futures)
- [ ] Add account health monitoring (balance, drawdown)

**Phase 4: Advanced Features (ongoing)**
- [ ] Portfolio rebalancing suggestions
- [ ] Risk analytics (correlation across accounts)
- [ ] Tax reporting (crypto gains tracking)
- [ ] Social features (connect with other FKS prop traders)

### Testing Strategy

**Unit Tests** (`src/services/ninja/tests/test_wallet_tracker.py`):
```python
import pytest
from wallets.hardware_wallet import HardwareWalletTracker

@pytest.mark.asyncio
async def test_xpub_balance_query():
    # Use testnet xpub
    tracker = HardwareWalletTracker(xpub='tpub...')
    balance = await tracker.get_balance()
    
    assert 'btc' in balance
    assert 'usd' in balance
    assert balance['btc'] >= 0
```

**Integration Tests** (`src/services/ninja/tests/integration/test_signal_execution.py`):
```python
@pytest.mark.integration
async def test_hyrotrader_signal_execution():
    # Use Bybit testnet
    executor = HyroTraderExecutor(
        api_key='testnet_key',
        api_secret='testnet_secret',
        testnet=True
    )
    
    signal = {
        'symbol': 'BTC/USDT:USDT',
        'action': 'long',
        'price': 50000,
        'tp_price': 51000,
        'sl_price': 49500
    }
    
    order = await executor.execute_signal(signal)
    assert order['status'] == 'open'
```

**NT8 Simulation Tests**:
- Run FKS_Listener on NinjaTrader SIM
- Send test signals via signal server
- Verify limit orders placed correctly
- Confirm TP/SL brackets attached

### Key Citations & Resources

**NinjaTrader Integration**:
- [Ninjatrader, IB, Python integration](https://forum.ninjatrader.com/forum/ninjatrader-8/platform-technical-support-aa/1091482-ninjatrader-ib-python-integration-backtest-and-live-trading-environment)
- [Using Python in NinjaTrader](https://forum.ninjatrader.com/forum/ninjatrader-8/add-on-development/1315563-using-python-in-ninjatrader)
- [Python.NET in NT8](https://forum.ninjatrader.com/forum/ninjatrader-8/add-on-development/1248249-python-runtime-in-ninjatrader-8)
- [CSharpNinja-Python Connector](https://github.com/TheSnowGuru/CSharpNinja-Python-NinjaTrader8-trading-api-connector-drag-n-drop)
- [Enhancing NT8 with Python](https://blog.stackademic.com/enhancing-ninjatrader-strategies-integrating-c-with-python-for-advanced-trading-automation-805a7f434513)

**Prop Firm Setup**:
- [NT8 Prop Firm Connection Guide](https://crosstrade.io/blog/ninjatrader-8-prop-firm-connection-guide/)
- [Rithmic Multi-Firm Setup](https://www.quantvps.com/blog/connect-to-different-rithmic-prop-firms-on-ninjatrader)
- [Take Profit Trader Tradovate Setup](https://www.youtube.com/watch?v=BXCA71YNq-w)
- [All Prop Firms with NinjaTrader](https://www.livingfromtrading.com/prop-firms/platforms/ninjatrader/)

**Crypto Prop Firms**:
- [HyroTrader Crypto API Trading](https://www.hyrotrader.com/blog/crypto-api-trading/)
- [HyroTrader Funded Programs Guide](https://www.hyrotrader.com/blog/funded-trader-programs/)
- [Crypto Funded Trader Program](https://www.hyrotrader.com/crypto-funded-trader/)
- [Best Crypto Prop Firms - Coinpedia](https://coinpedia.org/sponsored/best-crypto-prop-trading-firms/)

**Hardware Wallet Integration**:
- [Blockstream API Documentation](https://github.com/Blockstream/esplora/blob/master/API.md)
- [BIP32 HD Wallet Derivation](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki)
- [Ledger Developer Docs](https://developers.ledger.com/)

---

## ðŸ¤– Agent Usage Guide (Best Practices for AI Development)

### Core Development Philosophy

**Guiding Principles**:
1. **Test-Driven Development (TDD)** - Write tests BEFORE implementation
2. **Incremental Progress** - Small, focused changes over large rewrites
3. **Human Oversight** - Always request review for critical changes
4. **Explicit Over Implicit** - Ask for clarification when uncertain
5. **Edge Case Handling** - Consider failure modes proactively

### Agent Personas & Prompt Templates

**When to Use Each Persona**:

#### 1. Conservative Coder (Default for Core Systems)
**Use for**: Authentication, database migrations, payment logic, data integrity
**Prompt Template**:
```
As Conservative Coder, I need to [implement feature] in [file_path].

Requirements:
1. Write comprehensive tests FIRST (include edge cases: [specific examples])
2. Handle errors explicitly (no silent failures)
3. Add type hints and docstrings
4. Consider: What could go wrong? How do we recover?

Expected output: Tests â†’ Implementation â†’ Validation
```

**Example**:
```
As Conservative Coder, implement Redis caching in `feature_processor.py`.
Edge cases: connection failures, serialization errors, TTL expiry mid-request.
Tests first, then implementation with graceful degradation on cache miss.
```

#### 2. Performance Optimizer (For Speed-Critical Code)
**Use for**: Feature engineering, data processing pipelines, high-frequency operations
**Prompt Template**:
```
As Performance Optimizer, optimize [function/module] in [file_path].

Current performance: [metric] 
Target: [metric]

Constraints:
- Must maintain existing API
- Cannot break tests
- Benchmark before/after with [specific dataset]

Output: Profiling data â†’ Optimization â†’ Benchmark comparison
```

**Example**:
```
As Performance Optimizer, reduce quality check duration from 1s to <300ms.
Current: Z-score outlier detection on 10k rows.
Profile with cProfile, optimize NumPy operations, benchmark on BTCUSDT 1-week data.
```

#### 3. Integration Specialist (For Cross-Service Features)
**Use for**: API integration, service-to-service communication, external APIs
**Prompt Template**:
```
As Integration Specialist, integrate [system A] with [system B].

Data flow: [describe flow]
Failure modes: [list potential failures]
Fallback strategy: [e.g., cached responses, circuit breaker]

Output: Integration tests â†’ Implementation â†’ Error handling â†’ Monitoring
```

**Example**:
```
As Integration Specialist, integrate EODHD API with TimescaleDB storage.
Failure modes: rate limits (1000/day), network errors, malformed responses.
Fallback: Use cached data, alert on staleness >24h.
Tests must mock API calls.
```

### Interaction Workflow (Step-by-Step)

**For Complex Tasks** (e.g., "Implement multi-agent debate system"):

1. **Analysis Phase**
   ```
   Before implementing, I need to:
   - Clarify requirements: [specific questions]
   - Identify dependencies: [list services/modules]
   - Assess risks: [potential issues]
   
   Should I proceed with [proposed approach]?
   ```

2. **Test Planning**
   ```
   I'll write tests for:
   - Happy path: [describe]
   - Edge cases: [list specific scenarios]
   - Failure modes: [how system degrades]
   
   Confirm test strategy before implementation?
   ```

3. **Implementation**
   ```
   Implementing in phases:
   Phase 1: [small testable unit]
   Phase 2: [build on Phase 1]
   Phase 3: [integration]
   
   Running tests after each phase.
   ```

4. **Validation**
   ```
   Validation checklist:
   - [ ] All tests passing (show output)
   - [ ] Linting clean (`make lint`)
   - [ ] Performance acceptable ([metric])
   - [ ] Documentation updated
   
   Request final review?
   ```

### Asking for Alternatives & Debate

**When Uncertain, Request Options**:
```
I see two approaches for [problem]:

Option A: [description]
Pros: [list]
Cons: [list]

Option B: [description]  
Pros: [list]
Cons: [list]

Which aligns better with [project goal]? Or is there a third option?
```

**Debugging Strategy**:
```
Issue: [describe problem]

Investigation:
1. Checked: [what I verified]
2. Found: [specific error/behavior]
3. Hypothesis: [what I think is wrong]

Should I:
A) [proposed fix]
B) [alternative approach]
C) Provide more diagnostic info first?
```

### Ethical AI Checks (For Trading Strategies)

**Before Implementing Trading Logic, Ask**:
```
Ethical Review for [strategy]:

1. Market Manipulation: Could this create wash trades or spoofing?
   - Check: [specific safeguards]

2. Fairness: Does this exploit informational asymmetry ethically?
   - Justification: [why this is acceptable]

3. Risk Management: What's max exposure/drawdown?
   - Limits: [hard stops]

4. Transparency: Can we explain decisions to regulators?
   - Documentation: [audit trail]

Proceed if all checks pass.
```

### Human Review Mandates

**ALWAYS request human review for**:
- Authentication/security changes
- Database schema modifications
- Trading execution logic (order placement)
- Configuration affecting production systems
- Large refactors (>500 lines changed)

**Prompt Template**:
```
âš ï¸ HUMAN REVIEW REQUIRED

Changes: [summary]
Files: [list modified files]
Risk: [potential impact]

Please review before I:
- [ ] Commit changes
- [ ] Run migrations
- [ ] Deploy to production
```

### Best Practices Integration

**Data Quality (Per LuxAlgo/QuantStart)**:
- Pre-trade validation: Outlier detection, sanity tests
- Backtests require out-of-sample validation
- Walk-forward optimization to avoid overfitting

**Risk-Adjusted Metrics (Focus on Real Performance)**:
- Sharpe, Calmar, Sortino > raw returns
- Acknowledge uncertainty: "Backtests suggest X, but real-world may vary"
- Include forward test degradation estimates (50-70% from backtest)

**Modular Architecture (Already Implemented)**:
- Services communicate via HTTP APIs only
- No cross-service imports
- Clear service boundaries per table above

### FKS Repository Structure (Monorepo)

```
fks/  (THIS REPOSITORY)
â”œâ”€â”€ docker-compose.yml         # 8-service orchestration
â”œâ”€â”€ docker-compose.gpu.yml     # GPU overrides for fks_ai (Ollama)
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ Makefile                   # Dev commands (make up, make gpu-up)
â”œâ”€â”€ manage.py                  # Django management
â”‚
â”œâ”€â”€ src/                       # All source code
â”‚   â”œâ”€â”€ services/             # Microservices (each has own README)
â”‚   â”‚   â”œâ”€â”€ api/              # fks_api - FastAPI gateway
â”‚   â”‚   â”œâ”€â”€ app/              # fks_app - business logic
â”‚   â”‚   â”œâ”€â”€ ai/               # fks_ai - GPU ML/RAG
â”‚   â”‚   â”œâ”€â”€ data/             # fks_data - market data collection
â”‚   â”‚   â”œâ”€â”€ execution/        # fks_execution - Rust order execution
â”‚   â”‚   â”œâ”€â”€ ninja/            # fks_ninja - .NET NinjaTrader bridge
â”‚   â”‚   â””â”€â”€ web/              # fks_web - Django UI
â”‚   â”‚
â”‚   â”œâ”€â”€ monitor/              # Service registry & health (fks_main)
â”‚   â”œâ”€â”€ authentication/       # Centralized auth (fks_main)
â”‚   â”œâ”€â”€ core/                 # Core models, exceptions, database
â”‚   â”œâ”€â”€ framework/            # Middleware, config, patterns
â”‚   â””â”€â”€ tests/                # Unit/integration tests
â”‚
â”œâ”€â”€ docs/                      # Comprehensive documentation
â”‚   â”œâ”€â”€ AI_STRATEGY_INTEGRATION.md      # 5-phase AI plan
â”‚   â”œâ”€â”€ CRYPTO_REGIME_BACKTESTING.md    # Regime detection research
â”‚   â”œâ”€â”€ ARCHITECTURE.md                  # Detailed architecture
â”‚   â”œâ”€â”€ PHASE_STATUS.md                  # Current status tracker
â”‚   â””â”€â”€ PHASE_*.md                       # Development phase plans
â”‚
â”œâ”€â”€ monitoring/                # Prometheus/Grafana config
â”‚   â”œâ”€â”€ prometheus/           # Metrics + alert rules
â”‚   â””â”€â”€ grafana/              # Dashboards (quality_monitoring.json)
â”‚
â”œâ”€â”€ sql/                       # TimescaleDB init scripts + migrations
â”œâ”€â”€ scripts/                   # Automation scripts
â””â”€â”€ tests/                     # Orchestrator tests
```

## ðŸ› ï¸ Developer Workflows

### Starting Services
```bash
# Standard stack (8 services)
make up              # Start all services
make logs            # Follow logs
make down            # Stop everything
make restart         # Restart

# GPU stack (adds Ollama + fks_ai)
make gpu-up          # Combines docker-compose.yml + docker-compose.gpu.yml
```

**Access Points**:
- **Health Dashboard**: http://localhost:8000/health/dashboard/
- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090
- Django Admin: http://localhost:8000/admin
- Flower (Celery): http://localhost:5555

### Database & Shell Commands
```bash
make migrate         # Run Django migrations
make shell           # Django shell (ORM access)
make db-shell        # PostgreSQL psql shell
```

### Testing (TDD Approach)
```bash
# Run tests in Docker containers (NOT on host)
docker-compose exec fks_app pytest tests/unit/strategies/asmbtr/ -v  # ASMBTR (108)
docker-compose exec fks_app pytest tests/unit/validators/ -v          # Quality (34)
docker-compose exec fks_app pytest tests/unit/cache/ -v              # Redis (20)
docker-compose exec fks_app pytest tests/unit/metrics/ -v            # Monitoring (40)

# Service-specific tests
docker-compose exec fks_api pytest tests/
docker-compose exec fks_data pytest tests/

# With coverage
docker-compose exec fks_app pytest --cov=strategies.asmbtr --cov-report=term-missing
```

**Test Markers** (`pytest.ini`): unit, integration, slow, data, backtest, trading, api, web

**Current Status**: 188/188 tests passing (100% coverage for implemented features)

### Code Quality
```bash
make lint            # ruff + mypy + black
make format          # auto-format with black + isort
```

### Dynamic Project Visualization with Mermaid ðŸ“Š

**Overview**: Mermaid enables text-based diagramming that evolves with your project, providing clarity amid growing complexity. Diagrams are version-controlled alongside code, eliminating manual update overhead.

**Why Mermaid for FKS**:
- **Architecture clarity**: Visualize 8-service microservices, agent flows, data pipelines
- **Phase tracking**: Generate Gantt charts for roadmap progress
- **Auto-updates**: Diagrams sync with code changes via CI/CD automation
- **Reduced onboarding**: 30% faster developer ramp-up with live visuals (per surveys)
- **Documentation drift prevention**: 40% reduction in outdated diagrams (agile environments)

#### Quick Start: Basic Integration

**Installation**:
```bash
pip install mermaid-py  # Python wrapper for Mermaid
npm install -g @mermaid-js/mermaid-cli  # For CI/CD rendering
```

**Example 1: Service Architecture Diagram**:
```python
from mermaid import Mermaid

# Dynamic flowchart from FKS services
services_diagram = """
graph TD
    A[fks_main:8000] -->|orchestrates| B[fks_api:8001]
    B -->|routes| C[fks_app:8002]
    C -->|queries| D[fks_data:8003]
    C -->|executes| E[fks_execution:8004]
    C -->|AI inference| F[fks_ai:8006]
    F -->|LLM| G[ollama:11434]
    F -->|memory| H[(ChromaDB)]
    D -->|stores| I[(TimescaleDB)]
    style F fill:#f9f,stroke:#333,stroke-width:4px
    style G fill:#bbf,stroke:#333,stroke-width:2px
"""
Mermaid(services_diagram).to_png("docs/architecture/services_flow.png")
```

**Example 2: Multi-Agent AI Pipeline** (Phase 6):
```python
agent_graph = """
graph LR
    A[Market Data] --> B{Analysts}
    B --> C[Technical]
    B --> D[Sentiment]
    B --> E[Macro]
    B --> F[Risk]
    C --> G{Debate}
    D --> G
    E --> G
    F --> G
    G --> H[Bull Agent]
    G --> I[Bear Agent]
    H --> J[Manager]
    I --> J
    J --> K{Signal}
    K --> L[Reflection]
    L --> M[(ChromaDB Memory)]
"""
Mermaid(agent_graph).to_svg("docs/architecture/agent_pipeline.svg")
```

**Example 3: Data Flow ER Diagram**:
```python
data_flow = """
erDiagram
    EXCHANGE ||--o{ MARKET_DATA : provides
    MARKET_DATA ||--|| TIMESCALEDB : stored_in
    TIMESCALEDB ||--o{ ASMBTR_STRATEGY : queries
    ASMBTR_STRATEGY ||--|| SIGNAL : generates
    SIGNAL ||--|| EXECUTION : triggers
    EXECUTION ||--|| EXCHANGE : sends_order
"""
Mermaid(data_flow).to_png("docs/architecture/data_flow.png")
```

#### Dynamic Updates for Project Growth

**Automated Diagram Generation** (parse project structure):
```python
import os
from pathlib import Path

def generate_service_graph():
    """Auto-generate Mermaid from src/services/ directory"""
    services_dir = Path("src/services")
    services = [d.name for d in services_dir.iterdir() if d.is_dir()]
    
    syntax = "graph TD\n    A[FKS Platform] --> B{Services}\n"
    for i, service in enumerate(services):
        syntax += f"    B --> S{i}[{service}]\n"
    
    # Add port annotations from docker-compose.yml (parse YAML)
    # Add health status from monitoring API
    
    return syntax

# Run in CI/CD or as pre-commit hook
diagram = generate_service_graph()
Mermaid(diagram).to_svg("docs/architecture/services_auto.svg")
```

**Phase Progress Gantt Chart**:
```python
phase_gantt = """
gantt
    title FKS Development Roadmap (Phases 1-12)
    dateFormat YYYY-MM-DD
    section Phase 5
    Data Foundation       :done, p5, 2025-10-01, 2025-10-15
    section Phase 6
    Multi-Agent AI        :done, p6, 2025-10-16, 2025-10-30
    section Phase 7
    Evaluation Framework  :active, p7, 2025-10-31, 2025-11-15
    LLM-Judge & RAG       :p71, 2025-11-16, 2025-11-30
    section Phase 8
    Walk-Forward Opt      :p8, 2025-12-01, 2025-12-20
"""
Mermaid(phase_gantt).to_png("docs/roadmap/phases_progress.png")
```

#### Integration with FKS Services

**Location**: `src/services/web/src/visualization/` (new module)

**Files**:
- `diagram_generator.py`: Core logic for dynamic Mermaid syntax generation
- `templates/mermaid/`: Reusable diagram templates (services, agents, data flows)
- `scripts/update_diagrams.py`: CLI for manual regeneration

**Web UI Integration** (Django/Vite):
```javascript
// In fks_web frontend (src/services/web/src/vite/)
import mermaid from 'mermaid';

mermaid.initialize({ 
    startOnLoad: true,
    theme: 'dark',  // Match FKS UI theme
    flowchart: { curve: 'basis' }
});

// Fetch diagram syntax from Django API
fetch('/api/diagrams/architecture')
    .then(res => res.json())
    .then(data => {
        const element = document.getElementById('diagram-container');
        mermaid.render('diagram-svg', data.mermaid_syntax, element);
    });
```

**Django Endpoint** (`src/services/web/src/api/views.py`):
```python
from django.http import JsonResponse
from visualization.diagram_generator import generate_service_graph

def get_architecture_diagram(request):
    """Return live Mermaid syntax for current architecture"""
    syntax = generate_service_graph()  # Parses docker-compose, service registry
    return JsonResponse({'mermaid_syntax': syntax})
```

#### CI/CD Automation (GitHub Actions)

**Setup**: `.github/workflows/render_diagrams.yml`
```yaml
name: Auto-Render Mermaid Diagrams
on:
  push:
    branches: [main, develop]
    paths:
      - 'src/services/**'
      - 'docs/**/*.md'
      - 'docker-compose*.yml'
  pull_request:
    paths:
      - 'docs/**/*.md'

jobs:
  render-diagrams:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      
      - name: Install Mermaid CLI
        run: npm install -g @mermaid-js/mermaid-cli
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip'
      
      - name: Install mermaid-py
        run: pip install mermaid-py
      
      - name: Generate Dynamic Diagrams
        run: |
          python scripts/update_diagrams.py
          # Generates diagrams in docs/architecture/
      
      - name: Render Markdown Embedded Diagrams
        run: |
          find docs -name '*.md' -exec grep -l '```mermaid' {} \; | while read file; do
            # Extract Mermaid blocks, render to PNG/SVG
            mmdc -i "$file" -o "${file%.md}_diagram.png"
          done
      
      - name: Commit Updated Diagrams
        uses: EndBug/add-and-commit@v9
        with:
          add: 'docs/architecture/*.png docs/architecture/*.svg'
          message: 'docs: Auto-update Mermaid diagrams [skip ci]'
          default_author: github_actions
```

**Alternative: Use Pre-built Action**:
```yaml
      - name: Render Mermaid in Markdown
        uses: neenjaw/render-md-mermaid@v2
        with:
          source-dir: docs/
          output-dir: docs/rendered/
          mermaid-version: '10.6.1'
```

**Benefits**:
- Diagrams update on every commit affecting services/architecture
- Pull requests show visual diffs in diagram outputs
- No manual intervention required for docs-as-code workflow

#### Best Practices for Clarity at Scale

**1. Modular Diagrams**: Avoid single mega-diagrams; split by concern
- `services_architecture.mmd`: High-level 8-service overview
- `agent_pipeline_detail.mmd`: Phase 6 multi-agent flow
- `data_pipeline.mmd`: fks_data â†’ TimescaleDB â†’ fks_app
- `monitoring_stack.mmd`: Prometheus â†’ Grafana â†’ Alertmanager

**2. Use Themes & Styling**:
```mermaid
%%{init: {'theme':'dark', 'themeVariables': {'primaryColor':'#ff6b6b'}}}%%
graph TD
    A[Critical Service] --> B[Dependency]
    style A fill:#f96,stroke:#333,stroke-width:4px
```

**3. Limit Complexity**: Max 50-100 nodes per diagram for performance
- For large systems, create drill-down hierarchy (overview â†’ service detail)

**4. Annotations**:
```mermaid
graph LR
    A[fks_app] -->|HTTP POST /analyze| B[fks_ai]
    B -->|~2s latency| C[Ollama LLM]
    C -->|llama3.2:3b| D[Signal Output]
```

**5. Version Control**: Commit `.mmd` source files alongside `.png` outputs
- Source: `docs/architecture/sources/services.mmd`
- Output: `docs/architecture/services.png`

#### Tools & Extensions

| Tool | Purpose | Integration |
|------|---------|-------------|
| **mermaid-py** | Python rendering | CI/CD scripts, Django views |
| **Mermaid CLI** | Command-line rendering | GitHub Actions, local dev |
| **VS Code Mermaid Extension** | Live previews | Developer workflow |
| **Streamlit** | Interactive dashboards | Admin tools for diagram editing |
| **Terramaid** | IaC to Mermaid | Future Terraform/K8s integration |

**VS Code Setup** (for instant previews):
```json
// .vscode/settings.json
{
    "markdown.mermaid.enabled": true,
    "markdown.preview.mermaid.theme": "dark"
}
```

#### Use Cases in FKS

**1. Onboarding**: New developers see `docs/QUICKSTART.md` with embedded diagrams
```markdown
# FKS Architecture Overview

```mermaid
graph TB
    subgraph "Core Services"
        main[fks_main:8000]
        api[fks_api:8001]
        app[fks_app:8002]
    end
    subgraph "Data Layer"
        data[fks_data:8003]
        db[(TimescaleDB)]
    end
    main --> api --> app --> data --> db
```
```

**2. Phase Planning**: Visualize task dependencies for Phases 7-12
**3. Debugging**: Trace data flows (e.g., EODHD API â†’ Redis cache â†’ fks_app)
**4. Documentation**: Architecture diagrams auto-update in `docs/ARCHITECTURE.md`

#### Performance & Scalability

**Rendering Times** (benchmarks):
- Simple graph (10 nodes): <1 second
- Medium (50 nodes): 2-5 seconds
- Complex (100+ nodes): 5-15 seconds (consider splitting)

**Optimization**:
- Cache rendered PNGs/SVGs, regenerate only on changes
- Use SVG for web (smaller, scalable), PNG for reports
- Lazy-load diagrams in web UI (only render visible sections)

#### Example: Auto-Generated Service Health Diagram

```python
# scripts/update_diagrams.py
import requests
from mermaid import Mermaid

def fetch_service_health():
    """Query fks_main health dashboard API"""
    response = requests.get('http://localhost:8000/health/services')
    return response.json()

def generate_health_diagram():
    services = fetch_service_health()
    syntax = "graph TB\n"
    
    for service in services:
        status_color = "#9f9" if service['status'] == 'healthy' else "#f99"
        syntax += f"    {service['name']}[{service['name']}<br/>Port: {service['port']}]\n"
        syntax += f"    style {service['name']} fill:{status_color}\n"
    
    # Add connections from service registry
    for service in services:
        for dep in service.get('dependencies', []):
            syntax += f"    {service['name']} --> {dep}\n"
    
    return syntax

# Run daily via cron or GitHub Actions
diagram = generate_health_diagram()
Mermaid(diagram).to_png("docs/monitoring/service_health.png")
```

This creates a **live architecture diagram** that reflects current service states, updating automatically with infrastructure changes.

**Key Citations**:
- [mermaid-py GitHub](https://github.com/ouhammmourachid/mermaid-py) - Python wrapper
- [Mermaid.js Official Docs](https://mermaid.js.org/) - Syntax reference
- [Diagrams as Code with Mermaid](https://www.freecodecamp.org/news/diagrams-as-code-with-mermaid-github-and-vs-code/) - freeCodeCamp guide
- [Auto-Updating Diagrams on Git Push](https://medium.com/@kanishks772/auto-updating-diagrams-the-magic-behind-git-push-30d1c7d80b84) - CI/CD integration
- [Terramaid: IaC to Mermaid](https://github.com/RoseSecurity/Terramaid) - Infrastructure visualization

---

## ðŸ“ Project-Specific Conventions

### File Organization
- **Django apps**: Directory-based under `src/` (e.g., `src/authentication/`, `src/trading/`)
- **Models**: `models.py` or `models/` subdirectory
- **Tests**: Mirror source structure in `tests/unit/` and `tests/integration/`
- **Naming**: `snake_case` for files/functions/variables; `PascalCase` for classes

### Import Patterns
```python
# Framework imports
from framework.middleware.circuit_breaker import CircuitBreaker
from framework.config.constants import TRADING_SYMBOLS

# Core models
from core.database.models import Trade, Position

# Django apps
from authentication.models import User
```

**Avoid**: Importing from `infrastructure/` or legacy `services/` modules

### Exception Hierarchy
All custom exceptions inherit from `FKSException` in `src/core/exceptions/__init__.py`:
- `TradingError`, `DataError`, `ModelError`, `ConfigError`
- `CircuitBreakerError`, `RateLimitError`
- Use specific exceptions, not generic `Exception`

### Configuration
- **Django**: Environment variables via `.env` â†’ `services/web/src/django/settings.py`
- **Framework**: Type-safe dataclasses in `src/framework/config/`
- **Service-specific**: Each microservice has own config (e.g., `services/api/config.py`)
- **Never hardcode**: API keys, secrets, DB credentials

## ðŸ”Œ Critical Integration Points

### TimescaleDB + pgvector
- Hypertables for time-series data (trades, market data)
- pgvector for RAG embeddings
- Extensions: `timescaledb,vector` in shared_preload_libraries

### Redis Caching
- Feature cache with TTL (1m=60s, 1h=3600s, 1d=86400s, 1w=604800s)
- EODHD API response caching (fundamentals=24h, earnings=1h)
- 80-95% performance improvement on repeated queries

### RAG System (GPU Stack)
- **Embeddings**: sentence-transformers (local) + OpenAI fallback
- **Vector Store**: pgvector in PostgreSQL
- **LLM**: Ollama/llama.cpp with CUDA
- **Purpose**: AI-powered trading intelligence and daily recommendations

### Monitoring Stack
- **Prometheus**: 10 quality metrics, 15s evaluation interval
- **Grafana**: 8-panel quality dashboard at http://localhost:3000
- **Alertmanager**: Discord webhook for critical alerts
- **TimescaleDB**: Continuous aggregates for hourly/daily statistics

### Discord Integration
- Trade notifications via `DISCORD_WEBHOOK_URL` env var
- Quality alerts, system errors, MDD breaches

## ðŸŽ¯ Current Development Priorities

### Phase 6: Multi-Agent Foundation (Weeks 1-4, 5-7 days each)

**Goal**: Establish LangGraph infrastructure for AI-powered trading decisions

**Phase 6.1: Agentic Foundation** (5-7 days)
- Setup LangChain/LangGraph with Ollama (llama3.2:3b)
- Define AgentState schema: market_data, signals, debates, memory
- Implement ChromaDB memory for agent interactions
- Create base agent factory with shared prompt templates

**Phase 6.2: Multi-Agent Debate** (7-10 days)
- **4 Analyst Agents**: Technical, Sentiment, Macro, Risk
- **3 Debate Agents**: Bull, Bear, Manager
- **3 Trader Personas + Judge**: Conservative, Moderate, Aggressive
- Adversarial debate loop with synthesis

**Phase 6.3: Graph Orchestration** (7-10 days)
- StateGraph: Analysts â†’ Debaters â†’ Manager â†’ Trader â†’ Reflection
- Conditional routing based on market regime
- Signal processor for unified outputs
- Reflection node for continuous learning

**Acceptance Criteria**:
- [ ] Ollama serving llama3.2:3b on fks_ai (8006)
- [ ] All 7 agents operational with debates generating contrasting views
- [ ] Graph execution <5 seconds, signal quality >60%
- [ ] ChromaDB memory functional with >1000 insights

**Code Location**: `services/ai/src/agents/`, `services/ai/src/graph/`, `services/ai/src/memory/`

---

### Medium-Term: Advanced AI (Weeks 5-12)

**Phase 7: Evaluation Framework** (3-5 days) - âœ… COMPLETE Oct 31, 2025
- âœ… Confusion matrices for ASMBTR/ML models - ModelEvaluator implemented
- âœ… Bonferroni & Benjamini-Hochberg p-value corrections
- âœ… Chi-square statistical testing
- â³ LLM-judge audits for factual consistency (Phase 7.2 next)
- â³ Ground truth backtests vs. reality (Phase 7.3)
- **Files**: confusion_matrix.py (502 lines), statistical_tests.py (219 lines), tests (461 lines)
- **Validation**: All 6 integration tests passing in fks_app container

**Phase 8: Hybrid Models & Risk** (5-7 days)
- LLM vetoes for signal validation
- Walk-forward optimization (WFO)
- MDD protection circuit breakers
- CPI-Gold hedging strategy

**Phase 9: Markov Integration** (5-7 days)
- Bull/bear/sideways state transitions
- Steady-state probability computation
- Hybrid ASMBTR+Markov signals

**Target Performance**: Calmar >0.45, Sharpe ~0.5, MDD <-0.5

---

### Long-Term: Production (Weeks 13-16)

**Phase 10: Deployment** (3-5 days)
- Grafana dashboards for agent metrics
- Discord alerts for critical events
- Resource limits and monitoring

**Phase 11: Iteration** (Ongoing)
- Paper trading (4 weeks)
- A/B testing (ASMBTR vs Hybrid vs Multi-agent)
- Ethics audit, v2 planning

**Target**: Real-world Sharpe >2.5 (vs. buy-hold 1.5)

## âš ï¸ Common Pitfalls & Troubleshooting

### Critical Rules
1. **Don't bypass fks_execution** - ONLY service that talks to exchanges/brokers
2. **Don't query exchanges directly** - Use fks_data for ALL market data
3. **GPU commands differ** - Use `make gpu-up` (combines base + GPU compose files)
4. **Cross-service imports forbidden** - Services communicate via HTTP APIs only
5. **Django settings location** - `services/web/src/django/settings.py`, NOT `src/web/django/`
6. **DJANGO_SETTINGS_MODULE** - Root `manage.py` sets to `services.web.src.django.settings`

### Common Issues & Solutions

| Issue | Symptom | Solution |
|-------|---------|----------|
| **Import Errors** | `ModuleNotFoundError: No module named 'config'` | Use `from framework.config.constants` instead |
| **Service Communication** | `ConnectionRefusedError` | Check `docker-compose ps`, use service name not localhost |
| **Test Discovery** | Pytest can't find tests | Files named `test_*.py` in `tests/` directory |
| **Docker Issues** | Services won't start | Check `.env` exists, run `make down && make up` |
| **Database Migrations** | Migration conflicts | Check `make migrate`, resolve in `src/*/migrations/` |
| **ASMBTR Tests** | Zero price changes failing | StateEncoder skips zero changes (not treated as DOWN) |
| **Container Resources** | Tests timeout/hang | Restart: `docker-compose down && docker-compose up -d` |

### ASMBTR Test Learnings (Oct 29, 2025)
- **Zero price changes**: Skipped entirely, not treated as DOWN movement
- **Attribute names**: Use `StateEncoder.encoder` (not `btr_encoder`), `PredictionTable.state_counts` (not `table`)
- **Dataclass properties**: `StatePrediction.prediction` is `@property`, not `__init__` param
- **API methods**: Use `save_to_dict()`/`load_from_dict()`, not `save()`/`load()`
- **Decay rate**: Must be 0.9-1.0, enforced in `PredictionTable.__init__`

### Getting Help
- **Logs**: `make logs` for error details
- **Health**: http://localhost:8000/health/dashboard/ for system status
- **Docs**: `docs/ARCHITECTURE.md` for detailed info
- **Tests**: `pytest tests/unit/test_X.py -v` to isolate issues

## ðŸ“š Documentation & Key Files

### Strategic Documentation
- **AI Enhancement Plan**: `.github/copilot-instructions.md` - 12-phase AI implementation
- **Phase Status**: `docs/PHASE_STATUS.md` - Current progress tracker
- **Architecture**: `docs/ARCHITECTURE.md` - Detailed system design
- **AI Strategy**: `docs/AI_STRATEGY_INTEGRATION.md` - Original 5-phase plan
- **Regime Research**: `docs/CRYPTO_REGIME_BACKTESTING.md` - Research analysis

### Operational Files
- **Django Settings**: `src/services/web/src/django/settings.py`
- **Root Management**: `manage.py` (sets DJANGO_SETTINGS_MODULE)
- **Docker Orchestration**: `docker-compose.yml`, `docker-compose.gpu.yml`
- **Database**: `sql/fundamentals_schema.sql`, `sql/migrations/`
- **Monitoring**: `monitoring/prometheus/`, `monitoring/grafana/dashboards/`
- **Testing**: `pytest.ini`, tests in `tests/unit/` and `src/services/*/tests/`

## ðŸ”§ When Making Changes

### Pre-Development Checklist
1. **Identify target service** - Determine which microservice needs changes
2. **Review service boundaries** - Ensure changes respect service responsibilities
3. **Check existing tests** - Understand test patterns before adding code
4. **Verify phase alignment** - Confirm work aligns with current Phase 6 goals

### TDD Workflow (Test-Driven Development)
1. **Write tests first** - Create test cases before implementation
2. **Implement changes** - Follow existing code patterns
3. **Run tests frequently** - `docker-compose exec <service> pytest tests/` after each change
4. **Validate syntax** - `make lint` to check style
5. **Format code** - `make format` for consistency

### Post-Development Checklist
1. **Run migrations** - `make migrate` after model changes
2. **Test locally** - `make up` + `make logs` to verify services
3. **Full test suite** - Run complete tests for affected service
4. **Check coverage** - Ensure >80% coverage for new code
5. **Update docs** - If changing architecture or major features
6. **Verify no regressions** - Ensure existing functionality still works

### Code Style & Quality
- **Formatting**: Black (line length 100) + isort
- **Type hints**: Add for function parameters and returns
- **Docstrings**: Google-style for classes and functions
- **Comments**: Only when code intent isn't obvious
- **DRY principle**: Extract repeated code into reusable components

## ðŸ“Š Test Status Summary

- âœ… **188/188 passing** - Complete test coverage for implemented features (Phase 5.6 complete, Oct 30, 2025)
- âœ… **ASMBTR Framework**: 108/108 passing (100%) - Baseline trading strategy fully validated
- âœ… **Redis Caching**: 20/20 passing (100%) - Feature cache and EODHD response caching
- âœ… **Quality Validators**: 34/34 passing (100%) - Outlier, freshness, completeness, scoring
- âœ… **Quality Monitoring**: 40/40 passing (100%) - Prometheus metrics, collector, TimescaleDB, E2E
- ðŸŽ¯ **Next Goal**: Phase 6 - Multi-agent AI system (LangGraph + Ollama + ChromaDB)

**Test Execution**:
```bash
# Run all test suites
docker-compose exec fks_app pytest tests/unit/strategies/asmbtr/ -v  # ASMBTR (108)
docker-compose exec fks_app pytest tests/unit/cache/ -v              # Redis (20)
docker-compose exec fks_app pytest tests/unit/validators/ -v         # Quality (34)
docker-compose exec fks_app pytest tests/unit/metrics/ -v            # Monitoring (40)

# With coverage
docker-compose exec fks_app pytest --cov=strategies.asmbtr --cov-report=term-missing
```

**Target**: 80%+ coverage across all 8 microservices

## Advanced AI Trading System Enhancement Plan (2025-2026)

### Overview: Hybrid Multi-Agent Architecture
**Goal**: Evolve FKS into a deep-thinking, self-improving trading system combining non-AI baselines (ASMBTR, Markov chains), multi-agent LLM architectures, advanced ML evaluation, risk controls, and hedging strategies for multi-asset crypto/FX trading.

**Timeline**: 12-16 weeks (phased rollout) | **Expected Performance**: Calmar >0.4, Sharpe ~0.5, Max Drawdown <-0.5

### Core Components & Technologies

**AI/ML Stack Enhancements**:
- **Multi-Agent System**: LangGraph orchestration with Bull/Bear/Manager agents, adversarial debates
- **Hybrid Models**: CNN-LSTM + LLM vetoes, WFO parameter optimization, MDD protection
- **Baselines**: ASMBTR (Adaptive State Model on BTR), Markov chains for memoryless transitions
- **Evaluation**: Confusion matrices, p-value adjustments (Bonferroni/Benjamini-Hochberg), LLM-judge audits
- **Memory**: ChromaDB for agent memory, Redis for state management
- **Hedging**: CPI-Gold threshold switching, multi-asset diversification

**Integration Points**:
- `fks_ai` (8006): LangGraph agents, regime detection, hybrid model inference
- `fks_app` (8002): ASMBTR strategies, Markov signal processing, portfolio optimization
- `fks_data` (8003): Multi-asset data (crypto/FX), CPI/Gold macro data ingestion
- `fks_execution` (8004): Dual pool execution (long/short), MDD circuit breakers

### 12-Phase Implementation Roadmap

#### Phase 1: Data Preparation & Compatibility (1-2 days)
**Objective**: Establish data foundation for ASMBTR and multi-agent systems

**Tasks**:
- Fetch high-frequency FX data (EUR/USD, GBP/USD) and crypto (BTC/ETH) via CCXT
- Implement micro-price change (Î”) scanner for tick-level analysis
- Verify TimescaleDB hypertable compatibility with BTR encoding
- Create `docs/ASMBTR_COMPATIBILITY.md` documenting data requirements

**Code Location**: `services/data/src/collectors/`, `services/data/src/processors/`

**Acceptance Criteria**:
- [ ] EUR/USD tick data streaming to TimescaleDB
- [ ] Î” scanner detecting price changes <0.01%
- [ ] Data quality report showing >99% completeness

---

#### Phase 2: ASMBTR Baseline Core (3-5 days)
**Objective**: Implement Adaptive State Model on BTR as memoryless baseline

**Tasks**:
- Build BTR (Binary Tree Representation) encoder: `asmbtr/btr.py`
- Implement state encoding: `asmbtr/encoder.py` with configurable depth (default: 8)
- Create prediction table: `asmbtr/predictor.py` mapping states â†’ probabilities
- Develop trading logic: `asmbtr/strategy.py` with event-driven execution
- Add configuration: Support variable depths, decay rates, threshold tuning

**Code Location**: `services/app/src/strategies/asmbtr/`

**Key Algorithm**:
```python
# BTR Encoding Example (depth=8)
def encode_asmbtr(deltas: List[float], depth: int = 8) -> str:
    """Encode price changes into binary tree state"""
    state = ""
    for delta in deltas[-depth:]:
        state += "1" if delta > 0 else "0"
    return state  # e.g., "10110011"

def predict_next(state: str, table: Dict) -> float:
    """Predict next move probability from learned table"""
    return table.get(state, {}).get('up_prob', 0.5)
```

**Acceptance Criteria**:
- [ ] BTR encoder handles variable depths (4-12)
- [ ] Prediction table populated from historical EUR/USD data
- [ ] Strategy achieves Calmar >0.3 on 2024 backtest

---

#### Phase 3: Baseline Testing & Optimization (4-7 days)
**Objective**: Validate ASMBTR and optimize hyperparameters

**Tasks**:
- Unit tests: `tests/unit/strategies/test_asmbtr.py` (>80% coverage)
- Integration tests: `tests/integration/test_asmbtr_backtest.py`
- Hyperparameter search with Optuna: depth, decay rate, entry/exit thresholds
- Compare ASMBTR vs. RSI/MACD baselines on multiple pairs (EUR/USD, BTC/USDT)
- Document results: `docs/ASMBTR_OPTIMIZATION.md`

**Code Location**: `tests/`, `scripts/optimize_asmbtr.py`

**Optuna Example**:
```python
def objective(trial):
    depth = trial.suggest_int('depth', 6, 12)
    decay = trial.suggest_float('decay', 0.95, 0.999)
    threshold = trial.suggest_float('threshold', 0.55, 0.75)
    
    strategy = ASMBTRStrategy(depth=depth, decay=decay, threshold=threshold)
    backtest = run_backtest(strategy, data='EUR_USD_2024')
    return backtest.calmar_ratio
```

**Acceptance Criteria**:
- [ ] Test coverage >80% for ASMBTR module
- [ ] Optimized hyperparameters documented
- [ ] Calmar ratio improvement >10% vs. default params

---

#### Phase 4: Baseline Deployment (2-3 days)
**Objective**: Containerize ASMBTR and integrate with Celery

**Tasks**:
- Update `docker/Dockerfile.app` with ASMBTR dependencies
- Create Celery task: `tasks/asmbtr_prediction.py` for periodic state updates
- Add monitoring: Prometheus metrics for state transitions, prediction accuracy
- Configure Celery Beat schedule: Run ASMBTR predictions every 1 minute

**Code Location**: `services/app/tasks/`, `monitoring/prometheus/`

**Acceptance Criteria**:
- [ ] ASMBTR runs in fks_app container
- [ ] Celery tasks executing every 60 seconds
- [ ] Prometheus dashboards showing live state metrics

---

#### Phase 5: Agentic Foundation (5-7 days)
**Objective**: Establish LangGraph infrastructure for multi-agent system

**Tasks**:
- Setup LangChain/LangGraph with local Ollama (llama3.2:3b for cost efficiency)
- Define AgentState schema: TypedDict with market_data, signals, debates, memory
- Implement ChromaDB memory: Store agent interactions, decisions, reflections
- Extend toolkit: Add CCXT tools, TA-Lib wrappers, backtesting utilities
- Create base agent factory: `agents/base.py` with shared prompt templates

**Code Location**: `services/ai/src/agents/`, `services/ai/src/memory/`

**AgentState Schema**:
```python
from typing import TypedDict, List, Annotated
from langgraph.graph import add_messages

class AgentState(TypedDict):
    messages: Annotated[List, add_messages]
    market_data: Dict[str, Any]  # OHLCV, indicators
    signals: List[Dict]  # Bull/Bear recommendations
    debates: List[str]  # Adversarial arguments
    memory: List[str]  # ChromaDB retrieval context
    final_decision: Optional[str]  # Manager output
```

**Acceptance Criteria**:
- [ ] Ollama serving llama3.2:3b on fks_ai (8006)
- [ ] ChromaDB initialized with trading knowledge base
- [ ] Base agent can query market data and generate simple signals

---

#### Phase 6: Multi-Agent Debate System (7-10 days)
**Objective**: Build adversarial agents with specialized roles

**Tasks**:
1. **Analyst Agents (4 types)**:
   - Technical Analyst: RSI, MACD, Bollinger analysis
   - Sentiment Analyst: News/social media (optional, use OpenAI fallback)
   - Macro Analyst: CPI, interest rates, correlations
   - Risk Analyst: VaR, MDD, position sizing

2. **Debate Agents**:
   - Bull Agent: Optimistic scenarios, long opportunities
   - Bear Agent: Pessimistic scenarios, short signals
   - Manager Agent: Synthesizes debates, final decision

3. **Trader Personas (3 + Judge)**:
   - Conservative: Low leverage, tight stops
   - Moderate: Balanced risk/reward
   - Aggressive: High leverage, wider stops
   - Judge: Selects best persona based on market regime

**Code Location**: `services/ai/src/agents/analysts/`, `services/ai/src/agents/debaters/`

**Debate Loop Example**:
```python
async def debate_node(state: AgentState):
    """Adversarial debate between Bull and Bear"""
    bull_arg = await bull_agent.invoke(state)
    bear_arg = await bear_agent.invoke(state)
    
    # Store debate for reflection
    state['debates'].extend([bull_arg, bear_arg])
    
    # Manager synthesizes
    decision = await manager_agent.invoke({
        **state,
        'bull_argument': bull_arg,
        'bear_argument': bear_arg
    })
    state['final_decision'] = decision
    return state
```

**Acceptance Criteria**:
- [ ] All 4 analyst agents operational
- [ ] Bull/Bear debates generating contrasting views
- [ ] Manager making decisions based on debate quality
- [ ] Judge selecting personas >70% accuracy on test data

---

#### Phase 7: Graph Orchestration & Reflection (7-10 days)
**Objective**: Build end-to-end StateGraph with feedback loops

**Tasks**:
- Construct StateGraph: Analysts â†’ Debaters â†’ Manager â†’ Trader â†’ Execution
- Add conditional edges: Route based on market regime (calm/volatile)
- Implement SignalProcessor: Aggregate multi-agent outputs into unified signals
- Build Reflector: Analyze past decisions, update memory with learnings
- Initial evaluations: Run graph on historical data, measure latency (<5s per decision)

**Code Location**: `services/ai/src/graph/`, `services/ai/src/processors/`

**Graph Structure**:
```python
from langgraph.graph import StateGraph, END

graph = StateGraph(AgentState)

# Nodes
graph.add_node("analysts", run_analysts)
graph.add_node("debate", debate_node)
graph.add_node("manager", manager_decision)
graph.add_node("trader", select_trader_persona)
graph.add_node("reflect", reflection_node)

# Edges
graph.add_edge("analysts", "debate")
graph.add_edge("debate", "manager")
graph.add_conditional_edges(
    "manager",
    route_to_trader,  # Checks market regime
    {"conservative": "trader", "aggressive": "trader", "skip": END}
)
graph.add_edge("trader", "reflect")
graph.add_edge("reflect", END)

compiled_graph = graph.compile()
```

**Acceptance Criteria**:
- [ ] Graph executes full pipeline in <5 seconds
- [ ] Conditional routing working based on volatility
- [ ] Reflection node updating ChromaDB with insights
- [ ] Signal quality >60% accuracy on validation set

---

#### Phase 8: Advanced Evaluation Framework (3-5 days)
**Objective**: Implement rigorous testing and validation

**Tasks**:
- Confusion Matrix: For ASMBTR and ML models (KNN-like in `fks_ai`)
  - Calculate precision, recall, F1 for buy/sell/hold predictions
  - Apply Bonferroni/Benjamini-Hochberg corrections for p-values
  - Test on BTC/ETH data (2023-2024)

- LLM-Judge Audits: Verify agent reasoning
  - Factual consistency checks (e.g., "Did Bull agent hallucinate data?")
  - Discrepancy detection between analyst claims and actual market data
  - Bias analysis (over-optimistic/pessimistic patterns)

- Ground Truth Backtests: Compare predictions vs. reality
  - ASMBTR states vs. actual next-price moves
  - Agent signals vs. optimal hindsight trades
  - CPI-Gold hedge vs. S&P 500 benchmark

**Code Location**: `tests/unit/test_evaluation.py`, `services/ai/src/evaluators/`

**Confusion Matrix Example**:
```python
from sklearn.metrics import confusion_matrix, classification_report
from scipy.stats import chi2_contingency

# Predictions: buy(1), sell(-1), hold(0)
y_true = [1, 1, -1, 0, 1, -1, 0, 0, 1, -1]
y_pred = [1, 0, -1, 0, 1, -1, 1, 0, 1, 0]

cm = confusion_matrix(y_true, y_pred)
print(classification_report(y_true, y_pred))

# Chi-square test
chi2, p_value, _, _ = chi2_contingency(cm)
adjusted_p = p_value * 3  # Bonferroni for 3 classes
print(f"Adjusted p-value: {adjusted_p}")
```

**Acceptance Criteria**:
- [ ] Confusion matrices showing balanced precision/recall (>0.6)
- [ ] LLM-judge catching >80% of factual errors in test cases
- [ ] Ground truth backtests validating ASMBTR Calmar >0.4

---

#### Phase 9: Hybrid Models & Risk Controls (5-7 days)
**Objective**: Integrate CNN-LSTM with LLM vetoes and hedging

**Tasks**:
1. **LLM Vetoes**:
   - Add veto layer to strategies: LLM reviews signals before execution
   - Prompt: "Given [market_data], [signal], assess risk. Veto if: ..."
   - Integrate with `fks_execution` to block high-risk trades

2. **Walk-Forward Optimization (WFO)**:
   - Implement rolling window parameter updates (monthly retraining)
   - Use Optuna for hyperparameter search on each window
   - Track parameter drift over time

3. **Maximum Drawdown Protection**:
   - Add circuit breaker in `fks_execution`: Halt trading if MDD > threshold (-15%)
   - Email alerts via Discord webhook
   - Auto-reduce position sizes during drawdown recovery

4. **CPI-Gold Hedging**:
   - Build `strategies/hedge.py` with threshold-based asset switching
   - Fetch CPI data from BLS API, Gold prices from yfinance
   - Corrected metrics: Target Sharpe 0.48 (not >1 as claimed), drawdown -0.51
   - Backtest vs. S&P 500 benchmark (2020-2024)

**Code Location**: `services/app/src/strategies/hybrid/`, `services/execution/src/risk/`

**LLM Veto Example**:
```python
async def veto_check(signal: Dict, market_data: Dict) -> bool:
    """LLM reviews signal for risk"""
    prompt = f"""
    Signal: {signal['action']} {signal['symbol']} at {signal['price']}
    Market: RSI={market_data['rsi']}, Vol={market_data['volatility']}
    
    Veto this trade if:
    1. RSI > 80 (overbought) and action = BUY
    2. Volatility > 2x average
    3. News sentiment extremely negative
    
    Answer: VETO or APPROVE
    """
    response = await ollama.generate(prompt)
    return "VETO" in response.upper()
```

**CPI-Gold Hedge** (Corrected):
```python
def cpi_gold_strategy(cpi_yoy: float, threshold: float = 3.0):
    """Switch between Gold and S&P based on CPI"""
    if cpi_yoy > threshold:
        return "GOLD"  # High inflation
    else:
        return "SPY"  # Normal conditions
    
# Historical Performance (2020-2024):
# Sharpe: 0.48, Max DD: -0.51, Correlation (CPI-Gold): 0.85
```

**Acceptance Criteria**:
- [ ] LLM vetoes blocking >30% of risky signals in validation
- [ ] WFO showing <10% parameter drift over 6-month test
- [ ] MDD protection triggering correctly in backtest stress scenarios
- [ ] CPI-Gold hedge outperforming SPY in high-inflation periods (2021-2022)

---

#### Phase 10: Markov Chains & Integration (5-7 days)
**Objective**: Add memoryless state transitions and finalize system

**Tasks**:
1. **Markov Chain Module**:
   - Build `strategies/markov.py` for bull/bear/sideways state transitions
   - Compute steady-state probabilities for long-term predictions
   - Validate memoryless assumption: Test autocorrelation on price data
   - Use with ASMBTR for hybrid state-based signals

2. **Full System Merge**:
   - Integrate Markov states into StateGraph (new node: "markov_regime")
   - Update agents to use Markov context in prompts
   - Combine ASMBTR (micro) + Markov (macro) for multi-timeframe analysis

3. **Final Optimizations**:
   - Latency tuning: Reduce graph execution to <3 seconds
   - Memory pruning: ChromaDB cleanup for old, irrelevant records
   - Model refinement: Retrain ML models with all collected data

**Code Location**: `services/app/src/strategies/markov/`, `services/ai/src/graph/`

**Markov Example**:
```python
import numpy as np

# Transition matrix: Bull, Bear, Sideways
P = np.array([
    [0.7, 0.2, 0.1],  # From Bull
    [0.3, 0.6, 0.1],  # From Bear
    [0.4, 0.3, 0.3]   # From Sideways
])

# Steady-state: Solve Ï€ = Ï€P
eigenvalues, eigenvectors = np.linalg.eig(P.T)
steady_state = eigenvectors[:, np.isclose(eigenvalues, 1)]
steady_state = steady_state / steady_state.sum()
print(f"Long-term probabilities: Bull={steady_state[0]:.2f}, Bear={steady_state[1]:.2f}")
```

**Acceptance Criteria**:
- [ ] Markov module computing steady-state for BTC data
- [ ] Autocorrelation tests confirming memoryless on short-term (<1h)
- [ ] Graph latency <3 seconds for full pipeline
- [ ] Hybrid ASMBTR+Markov achieving Calmar >0.45

---

#### Phase 11: Deployment & Monitoring (3-5 days)
**Objective**: Production-ready deployment with observability

**Tasks**:
- Update Docker Compose: Add resource limits for `fks_ai` (8GB RAM, 2 CPUs)
- Prometheus metrics:
  - Agent decision latency, debate quality scores
  - Sharpe/Calmar ratios (live tracking)
  - MDD, position counts, veto rates
- Grafana dashboards:
  - Multi-agent system overview (nodes, edges, execution times)
  - Strategy performance (ASMBTR, Markov, Hybrid)
  - Risk metrics (VaR, MDD, exposure)
- Email alerts: Discord webhook for critical events (MDD breach, system errors)
- Update `docs/MONITORING_README.md` with new dashboards

**Code Location**: `monitoring/grafana/dashboards/`, `monitoring/prometheus/rules/`

**Acceptance Criteria**:
- [ ] All services running with health checks passing
- [ ] Prometheus scraping AI metrics every 30 seconds
- [ ] Grafana dashboards showing live agent activity
- [ ] Discord alerts functional for test events

---

#### Phase 12: Iteration & Continuous Learning (Ongoing)
**Objective**: Establish feedback loop for long-term improvement

**Tasks**:
- Live simulation: Run system in paper trading for 4 weeks
- A/B testing: Compare ASMBTR vs. Hybrid vs. Multi-agent on different pairs
- Incorporate best practices from searches:
  - **Data quality**: LuxAlgo's validation pipelines
  - **AI ethics**: TWIML's fairness checks (avoid market manipulation)
  - **Modular platforms**: Biz4Group's microservice patterns (already in use)
- Plan v2 features:
  - Reinforcement learning (RL) for adaptive strategies
  - Multi-agent collaboration on cross-asset arbitrage
  - Explainability dashboards (SHAP values for ML, LLM reasoning chains)
- Document learnings: Update `docs/PHASE_7_FUTURE_FEATURES.md`

**Code Location**: `docs/`, `scripts/ab_testing.py`

**Acceptance Criteria**:
- [ ] Paper trading logs showing 30-day performance
- [ ] A/B test results documented with statistical significance
- [ ] Ethics audit completed (no wash trading patterns detected)
- [ ] v2 roadmap drafted with prioritized features

---

### Best Practices Integration

**From Industry Research** (LuxAlgo, QuantStart, Biz4Group, DataScience-PM, SoftKraft):
1. **Data Validation**: Implement pre-trade checks (outlier detection, sanity tests)
2. **Risk-Adjusted Metrics**: Focus on Sharpe, Calmar, Sortino (not just raw returns)
3. **Modular Architecture**: Already implemented via microservices âœ…
4. **Walk-Forward Optimization**: Avoid overfitting with rolling windows (40-50% reduction)
5. **Ethical AI**: No market manipulation, transparent decision-making
6. **Ground Truth Validation**: Always backtest with out-of-sample data
7. **Phase-Gated AI Projects**: Checkpoints for risk control (AWS AgentScore methodology)
8. **Data Drift Detection**: Monitor distribution shifts (20-30% accuracy preservation)
9. **Agent Memory Isolation**: Prevent state pollution (20-40% token reduction)
10. **Incremental Learning**: Reduce training time by 50-90%

**Key Pitfalls to Avoid**:
- **Overfitting**: Use confusion matrices, cross-validation, WFO
- **Black Swan Events**: CPI-Gold correlation breaks in crises (2008, 2020)
- **Memory Assumptions**: Markov memoryless fails for long-term trends
- **LLM Costs**: Prioritize local Ollama; use OpenAI only for critical tasks
- **Data Sparsity**: ASMBTR needs high-frequency data; ensure <1s resolution
- **Agent Loops**: Phase gates prevent costly A2A failures ($47K+ disasters documented)
- **Drift Neglect**: Undetected drift degrades models by 20-30%

---

## ðŸ”„ Phase Enhancement & Retrofit Tasks (Continuous Improvement)

While Phases 1-7.3 are complete, the following tasks enhance robustness, prevent technical debt, and align with AI project management best practices. Tasks are organized by phase for retrofit integration.

### Phase 5 Enhancements: Data Foundation (Retrofit)

**Objective**: Enhance adaptability to market changes and expand data coverage

#### Task 5.1.1: Data Drift Detection Pipeline âš ï¸ HIGH PRIORITY
**Description**: Add automated checks using alibi-detect to monitor distribution shifts in OHLCV data from CCXT. Trigger Prometheus alerts if drift exceeds thresholds (e.g., Kolmogorov-Smirnov test p<0.05).

**Implementation**:
- Location: `src/services/data/src/validators/drift_detector.py` (new)
- Integration: Hook into `collectors/` after data ingestion
- Monitoring: Add Prometheus metric `data_drift_score` with Grafana dashboard
- Alert: Discord webhook when drift detected

**Dependencies**: 
- Existing: `src/services/data/src/adapters/`, `monitoring/grafana/`
- New: `pip install alibi-detect`

**Effort**: 3-5 days

**Rationale**: Research shows undetected drift degrades AI model accuracy by 20-30%. Non-stationary crypto markets require continuous monitoring.

**Acceptance Criteria**:
- [ ] Drift detector running on 15-minute intervals
- [ ] Prometheus metric showing drift scores <0.2 (stable)
- [ ] Discord alert functional for drift >0.5
- [ ] Unit tests: `tests/unit/validators/test_drift_detector.py` (15+ tests)

**Testing**:
```python
# Simulate regime change (bull â†’ bear)
test_data = generate_regime_shift(start='bull', end='bear', n=1000)
drift_score = detector.detect(test_data)
assert drift_score > 0.7  # Should detect significant shift
```

---

#### Task 5.2.1: Path-Dependent Feature Kernels ðŸŽ¯ PERFORMANCE BOOST
**Description**: Incorporate signature kernels (signatory library) to compute path summaries like cumulative volatility, drawdowns, and momentum from OHLCV time series.

**Implementation**:
- Location: `src/services/app/src/features/feature_processor.py` (extend)
- New method: `process_signature_features(ohlcv_data, depth=3)`
- Features: 
  - Signature kernel (depth 2-4): Captures path interactions
  - Cumulative max drawdown over rolling windows
  - Path-dependent volatility (log-signature)
- Flag: `USE_SIGNATURE_KERNELS` in config (default: False for compatibility)

**Dependencies**:
- Existing: `notebooks/transformer/` for prototyping
- New: `pip install signatory iisignature`
- Test: `tests/unit/features/test_signature_kernels.py`

**Effort**: 4-7 days (including prototyping in notebooks)

**Rationale**: Quant research shows path-dependent features improve regime detection by 15%, yielding Sharpe ratio gains of 0.2-0.3 in backtests. Aligns with recent academic papers on volatility modeling.

**Acceptance Criteria**:
- [ ] Signature features added as optional in FeatureProcessor
- [ ] Notebook validation: `notebooks/transformer/signature_kernel_analysis.ipynb`
- [ ] Backtest on BTC 2023-2024: Sharpe improvement â‰¥10%
- [ ] Performance: <500ms overhead for 1000 candles

**Example**:
```python
from signatory import signature
from features.feature_processor import FeatureProcessor

processor = FeatureProcessor(use_signatures=True, sig_depth=3)
features = processor.process_ohlcv_features(btc_data, symbol='BTCUSDT')
# Adds 12 new signature features (depth=3 â†’ 3+3Â²+3Â³ = 39 terms, compressed to 12)
```

---

#### Task 5.3.1: On-Chain Data Schema Extension ðŸ”— CRYPTO FOCUS
**Description**: Extend TimescaleDB schema to include blockchain metrics (whale transactions, exchange flows, hash rate) from EODHD crypto feeds.

**Implementation**:
- Location: `sql/fundamentals_schema.sql` (extend)
- New tables:
  - `on_chain_transactions` (time, symbol, whale_threshold, transaction_volume, flow_direction)
  - `network_metrics` (time, symbol, hash_rate, difficulty, active_addresses)
- Migration: `sql/migrations/005_onchain_schema.sql`
- Collector: `src/services/data/src/collectors/onchain_collector.py`

**Dependencies**:
- Phase 5 EODHD adapter
- Test: `tests/integration/test_schema_validation_integration.py`

**Effort**: 2-4 days

**Rationale**: Crypto regime backtesting (docs/CRYPTO_REGIME_BACKTESTING.md) requires hybrid fundamental + on-chain data. Best practices from Biz4Group emphasize unified schemas for multi-asset platforms.

**Acceptance Criteria**:
- [ ] TimescaleDB hypertables created for on-chain data
- [ ] Sample data ingested for BTC, ETH (1 month)
- [ ] Query performance: <100ms for 1-week aggregates
- [ ] Grafana dashboard showing whale activity

---

### Phase 6 Enhancements: Multi-Agent AI System (Retrofit)

**Objective**: Production stability, memory optimization, and risk control for 7-agent system

#### Task 6.1.1: Agent-Specific Memory Isolation ðŸ§  EFFICIENCY BOOST
**Description**: Implement scratchpads in AgentState to separate short-term memory per agent team. Use ChromaDB collections with namespaces (e.g., `technical_analyst_scratchpad`, `risk_team_memory`).

**Implementation**:
- Location: `src/services/ai/src/agents/state.py` (extend)
- New field: `agent_scratchpads: Dict[str, List[str]]` in AgentState
- ChromaDB: Partition collections by agent_name
- Cleanup: Auto-expire scratchpad entries >24 hours old

**Dependencies**:
- Existing: `src/services/ai/src/graph/trading_graph.py`
- Test: `tests/unit/agents/test_memory_isolation.py`

**Effort**: 5-8 days

**Rationale**: CrewAI framework research shows memory isolation reduces token usage by 20-40% and prevents state pollution (e.g., Technical Analyst's indicators leaking into Sentiment Analyst's context).

**Acceptance Criteria**:
- [ ] Each agent has isolated scratchpad in AgentState
- [ ] ChromaDB partitioned by agent (7 collections)
- [ ] Token usage reduced by â‰¥20% in multi-agent workflows
- [ ] Unit tests: 25+ tests for isolation edge cases

**Example**:
```python
state = create_initial_state(symbol="BTCUSDT")
state['agent_scratchpads']['technical_analyst'] = ["RSI=65", "MACD bullish"]
state['agent_scratchpads']['risk_analyst'] = ["VaR=2.5%", "MDD=-15%"]
# Isolated - no cross-contamination
```

---

#### Task 6.2.1: Phase Gates for Agent-to-Agent Communication ðŸš¦ RISK CONTROL
**Description**: Add checkpoints in LangGraph StateGraph to validate context before routing between agents. Implement MCP (Model Context Protocol) standards for token limits, failure handling, and retry logic.

**Implementation**:
- Location: `src/services/ai/src/graph/trading_graph.py` (extend)
- New nodes: `validate_context_gate`, `check_token_limit_gate`
- Conditional edges: Skip agents if validation fails
- Monitoring: Add Prometheus metric `agent_gate_failures`

**Dependencies**:
- Existing: `api/routes.py`, `monitoring/prometheus/`
- Standards: MCP specification for A2A communication

**Effort**: 3-6 days

**Rationale**: AWS AgentScore research documents real-world agent loop disasters costing $47K+ from uncontrolled A2A calls. Phase gates prevent infinite loops and token explosions.

**Acceptance Criteria**:
- [ ] Gates active at 3 decision points (Analystsâ†’Debate, Debateâ†’Manager, Managerâ†’Trader)
- [ ] Token limit enforcement: Max 8K tokens per agent call
- [ ] Retry logic: Max 3 attempts before circuit breaker
- [ ] Monitoring: Grafana dashboard showing gate pass/fail rates

**Example**:
```python
def validate_context_gate(state: AgentState):
    """Phase gate: Check token usage and state validity"""
    if count_tokens(state['messages']) > 8000:
        return "skip_agent"  # Circuit breaker
    if not state.get('market_data'):
        return "retry_with_fallback"
    return "proceed"

graph.add_conditional_edges("analysts", validate_context_gate)
```

---

#### Task 6.3.1: Ollama Edge Optimization (Quantization) âš¡ PERFORMANCE
**Description**: Containerize llama3.2:3b with GGUF quantization (Q4_K_M) for 50% faster inference while maintaining 90-95% accuracy. Update Dockerfile.ai with optimized model loading.

**Implementation**:
- Location: `docker/Dockerfile.ai`, `docker-compose.gpu.yml`
- Quantization: Use `ollama pull llama3.2:3b-q4_k_m` (quantized version)
- Benchmark: Compare latency vs. full precision (target: <2s per agent call)
- Memory: Reduce from 2GB to ~1GB VRAM usage

**Dependencies**:
- Existing GPU stack: `docker-compose.gpu.yml`
- Performance tests: `tests/performance/test_ollama_latency.py`

**Effort**: 4-7 days (including benchmarking)

**Rationale**: Research shows GGUF Q4 quantization maintains 90-95% accuracy while halving inference time. Critical for real-time trading (<5s total latency).

**Acceptance Criteria**:
- [ ] Quantized model deployed in fks_ai container
- [ ] Inference latency: <2s per agent call (down from 3-4s)
- [ ] Accuracy degradation: <5% on validation set
- [ ] Memory usage: <1.5GB VRAM (down from 2GB)

**Benchmark**:
```bash
# Before (full precision)
docker-compose exec fks_ai ollama run llama3.2:3b "Analyze BTC sentiment"
# Time: 3.2s, Memory: 2.1GB

# After (quantized)
docker-compose exec fks_ai ollama run llama3.2:3b-q4_k_m "Analyze BTC sentiment"
# Time: 1.8s, Memory: 1.2GB
```

---

### Phase 7 Enhancements: Evaluation & Advanced Models (Active)

**Objective**: Strengthen LLM-Judge, add WFO, and prepare for hedging strategies

#### Task 7.1.2: Bonferroni Corrections for Multiple Comparisons âœ… PLANNED
**Description**: Extend `test_evaluation_framework.py` to include chi-square tests with Bonferroni/Benjamini-Hochberg p-value corrections for ASMBTR multi-asset evaluations.

**Implementation**:
- Location: `tests/unit/evaluation/test_statistical_corrections.py` (new)
- Methods: `apply_bonferroni()`, `apply_benjamini_hochberg()`
- Integration: Update `ModelEvaluator` in `confusion_matrix.py`

**Dependencies**:
- Phase 7.1 complete
- Reference: `docs/PHASE_7_1_COMPLETE.md`

**Effort**: 2-4 days

**Rationale**: Stanford research emphasizes corrections to prevent false positives in ML evaluations (Type I error rate <0.05). Critical when testing across 10+ trading pairs simultaneously.

**Status**: âœ… Already implemented in Phase 7.1 (Oct 31, 2025)

---

#### Task 7.2.2: LLM-Judge Factual Consistency Audits âœ… COMPLETE
**Description**: Create judge agent using Ollama to score outputs for factual consistency, discrepancies, and bias. Chain with ground truth validator. LLM-Judge leverages one LLM to evaluate outputs from another, automating consistency checks on strategy recommendations, market forecasts, and trading signals.

**Core Concept**: LLM-Judge acts as a "second opinion" mechanism, simulating debate-like reasoning where a judge model scrutinizes primary outputs against factual sources (e.g., earnings reports, SEC filings, on-chain metrics). Research shows 15-30% improvement in factual consistency over standalone inference.

**Implementation**:
- Location: `src/services/ai/src/evaluators/llm_judge.py`
- Methods: `verify_factual_consistency()`, `detect_discrepancies()`, `analyze_bias()`
- API: POST `/ai/judge/{consistency,discrepancy,bias}`
- **Judge Architecture**:
  - Single-judge (one LLM evaluator) for speed
  - Multi-judge ensemble for consensus on controversial predictions
  - Prompt-based evaluation with trading-specific rubrics

**Key Evaluation Criteria**:
- **Factual Accuracy**: Match outputs to real-time data (e.g., EODHD market data, BLS CPI)
- **Logical Coherence**: Ensure signal reasoning follows market realities
- **Neutrality**: Detect sentiment bias in market analyses
- **Compliance**: Avoid market manipulation claims, align with SEC guidelines

**Dependencies**:
- Phase 7.2 complete
- Reference: `docs/PHASE_7_2_COMPLETE_SUMMARY.md`

**Effort**: 5-10 days

**Rationale**: Anthropic research shows judge systems improve reliability by 30%, addressing hallucination risks in agent reasoning. In financial contexts, reduces errors in volatile environments where inaccurate data could lead to significant losses. AWS benchmarks show 25% reduction in trading signal errors with multi-perspective validation.

**Status**: âœ… Complete (Oct 31, 2025) - 592 lines, 3 endpoints, 20+ tests

---

#### Task 7.2.3: RAG-Enhanced LLM-Judge (Expansion) ðŸŽ¯ RECOMMENDED
**Description**: Integrate Retrieval-Augmented Generation (RAG) with existing LLM-Judge to provide contextual evidence from external sources (market data, SEC filings, economic indicators) before rendering judgments.

**Core Concept**: RAG addresses judge LLMs relying solely on parametric knowledge by introducing a retrieval step using vector databases. Judge retrieves relevant documents/data before evaluation, reducing hallucinations by 20-30% and improving agreement with human judges from 70% to 85%+.

**Implementation**:
- Location: `src/services/ai/src/evaluators/rag_judge.py` (new)
- Architecture:
  ```python
  # RAG-Judge Pipeline
  1. Primary model generates output (e.g., buy/sell signal)
  2. RAG retrieval: Query ChromaDB/Pinecone for relevant context
     - Market reports (EODHD API)
     - SEC filings (EDGAR database)
     - Economic indicators (BLS CPI, Fed rates)
  3. Judge LLM evaluates with retrieved context
  4. Output: Consistency score + evidence citations
  ```

**Retrieval Layer**:
- Vector DB: ChromaDB (existing) or Pinecone for indexing market documents
- Embeddings: sentence-transformers (existing) for cosine similarity
- Query modes:
  - Keyword search: Exact matches (ticker symbols, dates)
  - Semantic search: Broader context (sentiment analysis, regime shifts)
- Top-k retrieval: 3-5 most relevant chunks per evaluation

**Judge Prompting Template**:
```python
prompt = """
Given retrieved context:
{chunks}

Evaluate if the trading signal/output:
{agent_output}

is factually consistent based on:
1. Accuracy: Does it match retrieved data?
2. Completeness: Covers key factors from context?
3. Neutrality: Avoids bias/manipulation?

Score: [0-10]
Explanation: [cite specific chunks]
"""
```

**Integration Points**:
- Chain with existing `llm_judge.py`: RAG as preprocessing step
- Connect to ground truth validator (Phase 7.3) for cross-verification
- Multi-judge ensemble: GPT-4 + Claude + Ollama vote with shared RAG context

**Use Cases in Trading**:
- **Signal Verification**: Validate ASMBTR predictions against historical volatility data
- **Regime Detection**: Verify bull/bear shifts against CPI reports, unemployment data
- **Macro Analysis**: Cross-check forecasts with retrieved Fed meeting minutes
- **Earnings Audits**: Compare sentiment analysis to actual SEC 10-K filings

**Dependencies**:
- Phase 7.2 complete (LLM-Judge base)
- ChromaDB operational (Phase 6)
- EODHD adapter (Phase 5)
- Vector search infrastructure

**Effort**: 7-12 days
- Days 1-3: RAG retrieval layer (ChromaDB indexing, embedding pipeline)
- Days 4-6: Judge prompt engineering with context injection
- Days 7-9: Integration testing on historical trading signals
- Days 10-12: Meta-evaluation against human expert judgments

**Expected Impact**:
- Precision improvement: 10-20% based on Snowflake/Evidently AI benchmarks
- Reduced manual reviews: 80% reduction in compliance audits (Galileo AI)
- Hallucination reduction: 20-30% via grounded context
- Latency: <3s per judgment with optimized retrieval

**Acceptance Criteria**:
- [ ] RAG pipeline retrieves relevant context in <1s (top-5 chunks)
- [ ] Judge outputs include evidence citations from retrieved documents
- [ ] Precision vs. human experts: â‰¥80% agreement on test set
- [ ] Integration tests: 20+ scenarios (regime shifts, earnings surprises)
- [ ] Grafana dashboard: RAG retrieval quality, judge latency metrics

**Best Practices**:
- Hybrid retrieval: Combine keyword (exact) + semantic (context)
- Chunk size: 256-512 tokens for trading docs (balance context vs. noise)
- Irrelevant filtering: Use reranking models (e.g., Cohere rerank) to reduce noise
- Primary sources: Prioritize SEC filings, BLS data over news articles
- Human-in-loop: Meta-evaluation every 100 judgments for drift detection

**Challenges & Mitigations**:
| Challenge | Mitigation |
|-----------|------------|
| Retrieval quality (irrelevant chunks) | Use reranking + metadata filters (date, ticker) |
| Scalability (high latency) | Cache frequent queries, batch evaluations |
| Cost overruns (API calls) | Use local Ollama for judge, limit to critical signals |
| Bias from retrieved data | Diverse sources, temporal coverage (avoid recency bias) |

**Python Implementation Sketch**:
```python
from evaluators.llm_judge import LLMJudge
from memory.chroma_memory import ChromaMemory
import requests

class RAGJudge(LLMJudge):
    def __init__(self, chroma_memory: ChromaMemory, eodhd_api_key: str):
        super().__init__()
        self.memory = chroma_memory
        self.eodhd_key = eodhd_api_key
    
    def evaluate_with_rag(self, agent_output: str, symbol: str, 
                          criteria: List[str]) -> Dict:
        # 1. Retrieve context
        query = f"Market data for {symbol}: {agent_output[:200]}"
        chunks = self.memory.query(query, top_k=5)
        
        # 2. Fetch real-time data
        fundamentals = self._fetch_eodhd_fundamentals(symbol)
        
        # 3. Build context
        context = "\n".join([c['content'] for c in chunks])
        context += f"\nReal-time PE: {fundamentals['PE']}, EPS: {fundamentals['EPS']}"
        
        # 4. Judge evaluation
        prompt = self._build_judge_prompt(agent_output, context, criteria)
        score, explanation = self._evaluate(prompt)
        
        return {
            'score': score,
            'explanation': explanation,
            'evidence': [c['metadata'] for c in chunks],
            'fundamentals': fundamentals
        }
```

**Key Citations**:
- [Evaluating RAG with LLM-as-a-Judge](https://www.nb-data.com/p/evaluating-rag-with-llm-as-a-judge) - Ragas framework
- [LLM As a Judge: Tutorial](https://www.patronus.ai/llm-testing/llm-as-a-judge) - Patronus AI best practices
- [ConsJudge: Improving RAG Evaluation](https://aclanthology.org/2024.findings-acl.1/) - ACL 2024 consistency-aware judges
- [LLM-Judge for Financial Governance](https://galileo.ai/blog/llm-as-a-judge-the-missing-piece-in-financial-services-ai-governance) - 80% reduction in compliance reviews
- [Snowflake RAG Benchmarking](https://www.snowflake.com/blog/llm-judge-rag-triad-metrics/) - 10-20% precision gains

**Status**: ðŸŽ¯ Recommended expansion (builds on Phase 7.2 foundation)

---

#### Task 7.2.4: Detailed LLM-Judge Subtasks (Modular Expansion) ðŸ“‹ OPTIONAL
**Description**: Break down LLM-Judge into granular subtasks for phased implementation, building on existing Phase 7.2 foundation.

**Subtask Breakdown**:

| Subtask ID | Description | Dependencies | Effort | Expected Impact |
|------------|-------------|--------------|--------|-----------------|
| 7.2.1.1 | Design Judge Prompt Templates | src/services/ai/src/agents/; PHASE_7_2 complete | 2-3d | Standardizes evaluations; 20% consistency improvement (AWS benchmarks) |
| 7.2.1.2 | Integrate Meta-Evaluation Metrics | tests/unit/evaluation/; Add AUC, precision for judge outputs | 3-5d | Enables benchmarking vs. human audits; 15% reliability gains |
| 7.2.1.3 | Add Debate Mechanism | graph/trading_graph.py; Chain bull/bear agents with judge | 4-7d | Multi-perspective validation; 25% error reduction in signals |
| 7.2.1.4 | Production Safeguards (Token Limits) | api/routes.py; Monitor via Grafana | 2-4d | Prevents cost overruns; essential for scalable finance AI |
| 7.2.1.5 | Fine-Tune for Trading Domain | Use Ollama; Train on SEC filings/market data | 5-10d | 30% domain accuracy boost (Galileo AI) |

**Subtask 7.2.1.1: Judge Prompt Templates** (2-3 days)
```python
# Example templates for different evaluation types
FACTUAL_CONSISTENCY_TEMPLATE = """
Evaluate the following trading signal for factual accuracy:

Signal: {agent_output}
Symbol: {symbol}
Timeframe: {timeframe}

Check against:
1. Historical price data: {price_context}
2. Fundamental metrics: {fundamentals}
3. Economic indicators: {macro_data}

Score (0-10): 
Explanation:
Inconsistencies found:
"""

BIAS_DETECTION_TEMPLATE = """
Analyze the following market analysis for sentiment bias:

Analysis: {agent_output}

Criteria:
1. Over-optimistic language (bull bias)
2. Over-pessimistic language (bear bias)
3. Balanced presentation of risks/opportunities

Bias Score (0=neutral, +10=extreme bull, -10=extreme bear):
Evidence:
"""
```

**Subtask 7.2.1.3: Debate Mechanism** (4-7 days)
- Integrate with existing Bull/Bear/Manager agents from Phase 6
- Judge evaluates debate outputs for factual grounding
- Consensus scoring: Weight judge evaluation in manager decision

**Meta-Evaluation Best Practices**:
- Benchmark against human expert judgments (finance professionals)
- Track judge-human agreement rates (target: â‰¥80%)
- Regular calibration: Every 100 evaluations, sample for human review
- A/B testing: Compare judge variants (single vs. ensemble, with/without RAG)

**Production Deployment Considerations**:
- Token budgets: Max 8K tokens per judgment (prevent cost explosions)
- Rate limiting: Cap at 100 judgments/hour (prevent API overload)
- Circuit breakers: Halt if judge accuracy drops below 70% agreement
- Monitoring: Grafana dashboards for judge latency, accuracy trends, cost per judgment

**Status**: ðŸ“‹ Optional modular expansion (for teams needing granular milestones)

---

#### Task 7.3.2: Ground Truth Validation Framework âœ… COMPLETE
**Description**: Compare agent predictions to optimal trades calculated with perfect hindsight. Provide objective performance metrics.

**Implementation**:
- Location: `src/services/ai/src/evaluators/ground_truth.py` (890 lines)
- Methods: 7 core methods including ChromaDB/TimescaleDB integration
- API: POST `/ai/validate/ground-truth`

**Dependencies**:
- Phase 7.3 complete
- Reference: `docs/PHASE_7_3_GROUND_TRUTH_COMPLETE.md`

**Effort**: 2-3 hours

**Status**: âœ… Complete (Oct 31, 2025) - 1,909 lines total, 16 tests passing

---

#### Task 7.4.1: Walk-Forward Optimization (WFO) Module ðŸŽ¯ NEXT PRIORITY
**Description**: Build WFO in strategies using rolling windows. Test on BTC 2020-2024 data to avoid overfitting, retraining monthly with Optuna.

**Implementation**:
- Location: `src/services/app/src/strategies/wfo/walk_forward.py` (new)
- Window: 6-month training, 1-month validation, 1-month rolling
- Optimization: Integrate with `examples/optimize_strategy.py`
- Metrics: Track parameter drift, validation Sharpe vs. training Sharpe

**Dependencies**:
- Ground truth results (Phase 7.3)
- Reference: `docs/CRYPTO_REGIME_BACKTESTING.md`
- Tool: Optuna for hyperparameter search

**Effort**: 7-14 days

**Rationale**: QuantStart recommends WFO as gold standard for realistic backtests. Reduces curve-fitting by 40-50%, critical for crypto's non-stationary nature.

**Acceptance Criteria**:
- [ ] WFO module operational with configurable windows
- [ ] BTC backtest: Out-of-sample Sharpe â‰¥70% of in-sample (acceptable degradation)
- [ ] Parameter drift tracking: Alert if >30% change month-over-month
- [ ] Integration with ground truth: Auto-tune based on efficiency ratio
- [ ] Unit tests: 30+ tests for edge cases (insufficient data, regime changes)

**Example**:
```python
from strategies.wfo import WalkForwardOptimizer

wfo = WalkForwardOptimizer(
    strategy='asmbtr',
    train_window='6M',
    validation_window='1M',
    step='1M'
)

results = wfo.optimize(
    data=btc_2020_2024,
    params={'depth': [6, 8, 10, 12], 'threshold': [0.55, 0.65, 0.75]},
    metric='calmar_ratio'
)

# Output: Monthly optimal params, degradation report
# Expected: In-sample Calmar 0.45 â†’ Out-sample 0.32 (71% retention - good!)
```

---

### Phase 8 (Proposed): Advanced Strategies & Hedging

**Objective**: Multi-asset diversification and black swan protection

#### Task 8.1.1: CPI-Gold Hedging Strategy ðŸ“ˆ CRISIS RESILIENT
**Description**: Integrate BLS CPI API and yfinance Gold data. Implement threshold-based switching: if CPI YoY >3%, allocate to Gold; else S&P 500.

**Implementation**:
- Location: `src/services/app/src/strategies/hedge.py` (new)
- Data sources:
  - BLS CPI: `https://api.bls.gov/publicAPI/v2/timeseries/data/CUUR0000SA0`
  - Gold: `yfinance.Ticker('GLD').history()`
- Backtest: 2020-2024 vs. S&P 500 benchmark
- Target: Sharpe 0.48, MDD -0.51 (corrected from original >1 claim)

**Dependencies**:
- Market data: `src/services/data/src/collectors/`
- Ground truth: Validate with Phase 7.3 framework

**Effort**: 6-9 days

**Rationale**: LuxAlgo research notes 20-30% drawdown reduction in diversified portfolios during crises (2008, 2020). CPI-Gold correlation 0.85 in high-inflation periods.

**Acceptance Criteria**:
- [ ] CPI data ingested daily (1-day lag from BLS)
- [ ] Hedge strategy backtested on 2020-2024
- [ ] Outperformance vs. SPY in 2021-2022 (high inflation): â‰¥15%
- [ ] MDD improvement: -0.51 (hedge) vs. -0.65 (SPY baseline)
- [ ] Ground truth efficiency: â‰¥50% (captures half of optimal hedge profit)

**Warning**: CPI-Gold correlation breaks in black swans (2008 credit crisis, 2020 pandemic start). Include circuit breaker for correlation <0.3.

---

#### Task 8.2.1: Markov Regime Modules ðŸ”„ STATE TRANSITIONS
**Description**: Add steady-state computations for bull/bear/sideways transitions. Validate memoryless assumption with autocorrelation tests on EUR/USD hourly data.

**Implementation**:
- Location: `src/services/app/src/strategies/markov.py` (new)
- States: Bull (trend >2%), Bear (trend <-2%), Sideways (|trend| â‰¤2%)
- Transition matrix: Estimated from historical data
- Steady-state: Solve Ï€P = Ï€ for long-run probabilities
- Integration: Combine with ASMBTR for multi-timeframe signals

**Dependencies**:
- Notebooks: `notebooks/transformer/markov_regime_analysis.ipynb`
- Tests: `tests/unit/strategies/test_markov.py`

**Effort**: 4-7 days

**Rationale**: Academic papers show Markov models improve regime detection by 15% vs. simple trend filters. Useful for macro-timeframe context (daily/weekly).

**Acceptance Criteria**:
- [ ] Markov module computes steady-state for BTC, ETH, EUR/USD
- [ ] Autocorrelation tests confirm memoryless on <1h data (p>0.05)
- [ ] Backtest: Regime detection accuracy â‰¥70% vs. manual labels
- [ ] Hybrid ASMBTR+Markov: Calmar improvement â‰¥10% vs. ASMBTR alone

**Example**:
```python
import numpy as np
from strategies.markov import MarkovRegime

P = np.array([
    [0.7, 0.2, 0.1],  # Bull â†’ Bull, Bear, Sideways
    [0.3, 0.6, 0.1],  # Bear â†’ Bull, Bear, Sideways  
    [0.4, 0.3, 0.3]   # Sideways â†’ Bull, Bear, Sideways
])

markov = MarkovRegime(transition_matrix=P)
steady_state = markov.compute_steady_state()
# Output: [0.52, 0.31, 0.17] - Long-run: 52% bull, 31% bear, 17% sideways
```

---

## ðŸš€ Monorepo CI/CD Optimization for Solo Development

### Overview: Streamlining Microservices in a Monorepo

Managing a monorepo with 8 microservices (fks_main, fks_api, fks_app, fks_data, fks_execution, fks_ninja, fks_ai, fks_web) presents unique challenges for solo developers. Without proper automation, build times balloon, deployments become error-prone, and cognitive overhead increases. Research from DigitalOcean, Graphite, and Codefresh demonstrates that strategic CI/CD optimizations can reduce build times by 20-50% while improving scalability and maintainability.

**Core Principles**:
- **Selective Processing**: Only build/test/deploy affected services (40-60% fewer pipeline runs)
- **Parallel Execution**: Concurrent service builds (halves pipeline duration)
- **Aggressive Caching**: Docker layers + dependencies (30-50% faster builds)
- **Incremental K8s Migration**: Docker Compose for dev, gradual K8s rollout for production

### Quick Reference: Key Strategies

**Path-Based Selective Builds** (40-60% efficiency gain)
- Trigger jobs only on changed paths (`/services/fks_ai/**`)
- Use GitHub Actions `paths` filters or GitLab `rules: changes`
- Avoids full rebuilds, eliminates "build fatigue"

**Caching & Optimization** (30-50% speedup)
- Cache Docker layers: `cache-from: type=gha` in build-push-action
- Cache dependencies: `actions/cache` for pip/npm with hash-based keys
- Tools like Nx/Turborepo cache unchanged computations

**Parallel & Modular Execution** (halves duration)
- Matrix strategies: Build services concurrently (`strategy: matrix: service: [...]`)
- Reusable YAML templates: DRY principle, consistent environments
- Respect dependencies with `needs` keywords

**GitOps for K8s Migration** (zero-downtime deploys)
- ArgoCD/Flux for declarative sync from Git
- Kompose converts Compose files to K8s manifests (`kompose convert`)
- Helm charts for reusable service templates, easy rollbacks

**Monitoring & Automation** (25% less oversight)
- Slack/Email notifications for failures
- Dependabot auto-PRs for security updates
- Local testing with `act` before pushing

**Real-World Validation**:
- Faire's pipeline: Cut from 5,000 to modular YAML (LinkedIn case study)
- Spacelift benchmarks: 35% efficiency gains with GitOps
- Microsoft AKS docs: Selective triggers + parallel jobs for microservices

### Organization and Structure Fixes

#### Path-Based Triggers for Selective Builds âš¡ HIGH IMPACT

**Problem**: Full monorepo builds on every commit waste time and resources.

**Solution**: Implement GitHub Actions path filters to rebuild only affected services.

**Implementation**:
```yaml
# .github/workflows/build_services.yml
name: Build Microservices
on:
  push:
    branches: [main, develop]
    paths:
      - 'src/services/**'
      - 'docker/**'
      - 'requirements*.txt'

jobs:
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      services: ${{ steps.filter.outputs.changes }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            fks_api:
              - 'src/services/api/**'
            fks_app:
              - 'src/services/app/**'
            fks_ai:
              - 'src/services/ai/**'
            fks_data:
              - 'src/services/data/**'
  
  build:
    needs: detect-changes
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: ${{ fromJSON(needs.detect-changes.outputs.services) }}
    steps:
      - name: Build ${{ matrix.service }}
        run: |
          docker build -f docker/Dockerfile.${{ matrix.service }} \
            -t fks_${{ matrix.service }}:${{ github.sha }} \
            src/services/${{ matrix.service }}
```

**Benefits**:
- 40-60% reduction in unnecessary builds (Graphite research)
- Faster feedback loops for focused changes
- Reduced CI/CD costs on paid tiers

**FKS-Specific Application**:
- Trigger fks_ai rebuild only when `src/services/ai/**` changes
- Trigger fks_app rebuild for strategy updates in `src/services/app/src/strategies/**`
- Shared dependency changes (e.g., `requirements.txt`) rebuild all Python services

---

#### Shared Workflows with Modular YAML ðŸ”§ MAINTAINABILITY

**Problem**: Duplicated CI/CD logic across services creates maintenance overhead.

**Solution**: Centralize common steps in reusable workflow templates.

**Implementation**:
```yaml
# .github/workflows/reusable_test.yml
name: Reusable Test Workflow
on:
  workflow_call:
    inputs:
      service_name:
        required: true
        type: string
      python_version:
        required: false
        type: string
        default: '3.13'

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python_version }}
          cache: 'pip'
      
      - name: Install Dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements.dev.txt
      
      - name: Run Tests
        run: |
          pytest src/services/${{ inputs.service_name }}/tests/ \
            --cov=src/services/${{ inputs.service_name }}/src \
            --cov-report=xml
      
      - name: Upload Coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: ${{ inputs.service_name }}
```

**Usage in Service-Specific Workflows**:
```yaml
# .github/workflows/test_fks_ai.yml
name: Test fks_ai
on:
  push:
    paths: ['src/services/ai/**']

jobs:
  test:
    uses: ./.github/workflows/reusable_test.yml
    with:
      service_name: ai
      python_version: '3.13'
```

**Benefits**:
- DRY principle: Update test logic once, applies to all services
- Consistency: Uniform test environments across microservices
- Faster onboarding: New services inherit proven templates

---

#### Dependency Management with Monorepo Tools ðŸ“¦ ADVANCED

**Problem**: Interdependent services create circular dependencies and version conflicts.

**Solution**: Use workspace models or tools like Nx/Turborepo for Python monorepos.

**Implementation (Poetry Workspaces)**:
```toml
# pyproject.toml (root)
[tool.poetry]
name = "fks-platform"
version = "1.0.0"

[tool.poetry.dependencies]
python = "^3.13"

[tool.poetry.group.dev.dependencies]
pytest = "^8.0.0"
ruff = "^0.5.0"

# Define workspaces
[tool.poetry.workspace]
members = [
    "src/services/api",
    "src/services/app",
    "src/services/ai",
    "src/services/data"
]
```

**Benefits**:
- Unified dependency resolution across services
- Shared virtual environment reduces disk usage
- Simplified updates: `poetry update` affects all services

**Alternative: Nx for Advanced Caching**:
```bash
# Install Nx for Python monorepos
npm install -g nx

# Configure nx.json for task caching
nx run-many --target=test --all --parallel=4
```

**Expected Impact**: 30-40% faster local builds via computation caching (Nx benchmarks)

---

### CI/CD Automations to Maximize Efficiency

#### Caching Strategies ðŸš„ CRITICAL PERFORMANCE

**Docker Layer Caching**:
```yaml
- name: Build and Push Docker Image
  uses: docker/build-push-action@v5
  with:
    context: src/services/${{ matrix.service }}
    file: docker/Dockerfile.${{ matrix.service }}
    push: true
    tags: ghcr.io/nuniesmith/fks_${{ matrix.service }}:${{ github.sha }}
    cache-from: type=gha
    cache-to: type=gha,mode=max
```

**Python Dependency Caching**:
```yaml
- name: Cache Python Dependencies
  uses: actions/cache@v4
  with:
    path: |
      ~/.cache/pip
      .venv
    key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
    restore-keys: |
      ${{ runner.os }}-pip-
```

**Impact**: 30-50% build time reduction (Beon Tech benchmarks)

---

#### Parallel Job Execution âš¡ SPEED BOOST

**Matrix Strategy for Multi-Service Builds**:
```yaml
jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [api, app, ai, data, execution, ninja, web]
        include:
          - service: ai
            dockerfile: Dockerfile.ai
            gpu: true
          - service: execution
            dockerfile: Dockerfile.execution
            language: rust
      fail-fast: false  # Continue building other services if one fails
      max-parallel: 4   # Limit concurrency to avoid resource contention
    steps:
      - name: Build ${{ matrix.service }}
        run: |
          docker build -f docker/${{ matrix.dockerfile || 'Dockerfile.' + matrix.service }} \
            -t fks_${{ matrix.service }}:${{ github.sha }} .
```

**Dependency-Aware Ordering**:
```yaml
jobs:
  build-base:
    runs-on: ubuntu-latest
    steps:
      - name: Build Base Images
        run: docker build -t fks_base:latest -f docker/Dockerfile .
  
  build-services:
    needs: build-base  # Wait for base image
    strategy:
      matrix:
        service: [api, app, ai, data]
    steps:
      - name: Build Service
        run: docker build --build-arg BASE_IMAGE=fks_base:latest ...
```

**Impact**: Halves pipeline duration (CodeCapers case study)

---

#### Automated Testing and Deployment ðŸ§ª QUALITY GATES

**Comprehensive Test Pipeline**:
```yaml
name: CI Pipeline
on: [push, pull_request]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Lint with Ruff
        run: ruff check src/
  
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [api, app, ai, data]
    steps:
      - name: Run Unit Tests
        run: pytest src/services/${{ matrix.service }}/tests/unit/
  
  integration-tests:
    needs: [lint, unit-tests]
    runs-on: ubuntu-latest
    services:
      postgres:
        image: timescale/timescaledb:latest-pg16
      redis:
        image: redis:7-alpine
    steps:
      - name: Run Integration Tests
        run: pytest tests/integration/ --docker-compose
  
  deploy:
    needs: integration-tests
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to Staging
        run: kubectl apply -f k8s/staging/
```

**Staged Rollouts with Health Checks**:
```yaml
- name: Deploy with Canary
  run: |
    kubectl set image deployment/fks-app fks-app=fks_app:${{ github.sha }}
    kubectl rollout status deployment/fks-app --timeout=5m
    
    # Run smoke tests
    curl -f http://staging.fks.com/health || kubectl rollout undo deployment/fks-app
```

**Notifications on Failures**:
```yaml
- name: Notify on Failure
  if: failure()
  uses: 8398a7/action-slack@v3
  with:
    status: ${{ job.status }}
    text: 'Build failed for ${{ matrix.service }}'
    webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

---

### Migration to Kubernetes (Incremental Approach)

#### Hybrid Local/Prod Setup ðŸ”„ GRADUAL MIGRATION

**Phase 1: Docker Compose for Development** (Current State)
```yaml
# docker-compose.yml
services:
  fks_api:
    build: ./docker/Dockerfile.api
    ports: ["8001:8001"]
    depends_on: [postgres, redis]
```

**Phase 2: Convert with Kompose**
```bash
# Generate K8s manifests from Compose
kompose convert -f docker-compose.yml -o k8s/generated/

# Customize for production
cp k8s/generated/* k8s/production/
# Edit k8s/production/* to add resource limits, secrets, etc.
```

**Phase 3: Hybrid Deployment**
```yaml
# Local: docker-compose.yml (development)
# Staging: k8s/staging/ (testing K8s configs)
# Production: k8s/production/ (final rollout)
```

**Migration Timeline**:
1. Week 1-2: Convert non-critical services (fks_ninja, fks_web) to K8s staging
2. Week 3-4: Migrate data layer (fks_data, postgres, redis)
3. Week 5-6: Core services (fks_api, fks_app, fks_ai)
4. Week 7-8: Production rollout with monitoring

---

#### Helm for Service Management ðŸ“¦ PRODUCTION READY

**Create Helm Chart for FKS Platform**:
```bash
helm create fks-platform
```

**Chart Structure**:
```yaml
# fks-platform/values.yaml
replicaCount: 2

services:
  api:
    image: ghcr.io/nuniesmith/fks_api
    port: 8001
    resources:
      limits: { cpu: "1", memory: "1Gi" }
  
  app:
    image: ghcr.io/nuniesmith/fks_app
    port: 8002
    resources:
      limits: { cpu: "2", memory: "2Gi" }
  
  ai:
    image: ghcr.io/nuniesmith/fks_ai
    port: 8006
    gpu: true
    resources:
      limits: { nvidia.com/gpu: "1", memory: "8Gi" }
```

**Deploy with Helm**:
```bash
# Install
helm install fks-staging fks-platform/ -f values-staging.yaml

# Update
helm upgrade fks-staging fks-platform/ --set services.api.replicas=3

# Rollback
helm rollback fks-staging 1
```

**Benefits**:
- Version-controlled deployments
- Easy rollback (helm rollback)
- Environment-specific configs (dev/staging/prod)

---

### Automation Comparison Table

| Automation Area | Tool/Method | Implementation | Expected Benefit | Solo Dev Impact |
|-----------------|-------------|----------------|------------------|-----------------|
| **Selective Builds** | GitHub Actions path filters | `paths: ['src/services/ai/**']` | 40-60% fewer full runs | Focus on changed services only |
| **Caching** | Docker layer cache + pip cache | `cache-from: type=gha` | 30-50% faster builds | Critical for quick iterations |
| **Parallel Execution** | Matrix strategies | `strategy: matrix: service: [...]` | Halves pipeline duration | All services build simultaneously |
| **Testing** | Pytest + coverage + gates | `pytest --cov --cov-fail-under=80` | Early bug detection | Maintains quality solo |
| **Deployment** | build-push-action + kubectl | `docker/build-push-action@v5` | Zero-downtime updates | Automated production deploys |
| **Monitoring** | Slack notifications + Dependabot | `8398a7/action-slack@v3` | Reduces manual oversight | Alerts on failures |
| **K8s Migration** | Kompose + Helm | `kompose convert` | Gradual transition | No "big bang" rewrites |
| **Dependency Updates** | Dependabot PRs | `.github/dependabot.yml` | Keeps project secure | Auto-updates with tests |

---

### Real-World Implementation: FKS-Specific Workflow

**Complete Production Pipeline** (`.github/workflows/production.yml`):
```yaml
name: Production CI/CD
on:
  push:
    branches: [main]

jobs:
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      services: ${{ steps.filter.outputs.changes }}
    steps:
      - uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            api: 'src/services/api/**'
            app: 'src/services/app/**'
            ai: 'src/services/ai/**'
            data: 'src/services/data/**'
  
  build-and-test:
    needs: detect-changes
    if: ${{ needs.detect-changes.outputs.services != '[]' }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: ${{ fromJSON(needs.detect-changes.outputs.services) }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Cache Dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements*.txt') }}
      
      - name: Run Tests
        run: pytest src/services/${{ matrix.service }}/tests/ --cov
      
      - name: Build Docker Image
        uses: docker/build-push-action@v5
        with:
          context: src/services/${{ matrix.service }}
          push: true
          tags: ghcr.io/nuniesmith/fks_${{ matrix.service }}:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
  
  deploy-staging:
    needs: build-and-test
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to K8s Staging
        run: |
          kubectl set image deployment/fks-${{ matrix.service }} \
            fks-${{ matrix.service }}=ghcr.io/nuniesmith/fks_${{ matrix.service }}:${{ github.sha }} \
            --namespace=staging
          kubectl rollout status deployment/fks-${{ matrix.service }} -n staging
  
  smoke-tests:
    needs: deploy-staging
    runs-on: ubuntu-latest
    steps:
      - name: Run Smoke Tests
        run: |
          curl -f https://staging.fks.com/health || exit 1
          pytest tests/smoke/ --url=https://staging.fks.com
  
  deploy-production:
    needs: smoke-tests
    if: success()
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Deploy to Production
        run: helm upgrade fks-production fks-platform/ --set image.tag=${{ github.sha }}
      
      - name: Notify Success
        uses: 8398a7/action-slack@v3
        with:
          status: success
          text: 'Deployed ${{ github.sha }} to production'
```

**Expected Results**:
- Build time: 8-12 minutes (down from 20-30 without caching)
- Deployment frequency: Multiple times per day (vs. weekly manual deploys)
- Failure detection: <5 minutes (automated smoke tests)
- Rollback time: <2 minutes (helm rollback)

---

### Best Practices for Solo Developers

**1. Start Small, Automate Incrementally**:
- Week 1: Path-based triggers + caching
- Week 2: Parallel builds + automated tests
- Week 3: Staging deployments
- Week 4: Production pipeline with rollback

**2. Monitor Pipeline Costs**:
- Use GitHub Actions free tier (2,000 minutes/month)
- Optimize with `if: changed-files` conditions
- Cache aggressively to reduce build minutes

**3. Local Testing with `act`**:
```bash
# Test workflows locally before pushing
act -j build-and-test -s GITHUB_TOKEN=$GITHUB_TOKEN
```

**4. Document Runbooks**:
- Create `docs/deployment/RUNBOOK.md` with:
  - Manual rollback procedures
  - Common pipeline failures + fixes
  - Emergency deploy process

**5. Use Pre-commit Hooks**:
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    hooks:
      - id: ruff
      - id: ruff-format
```

---

### Key Citations & Resources

**Monorepo CI/CD Best Practices**:
- [How to Containerize Your Monorepo Application](https://www.digitalocean.com/community/tutorials/how-to-containerize-monorepo-apps) - DigitalOcean guide
- [Deploying only changed Microservices](https://www.reddit.com/r/devops/comments/rxzath/deploying_only_the_microservices_that_have_changes/) - Reddit DevOps community
- [CI/CD Pipelines for Microservices](https://codefresh.io/blog/ci-cd-pipelines-microservices/) - Codefresh comprehensive guide
- [Automating CI/CD in Kubernetes for Microservices](https://overcast.blog/automating-ci-cd-pipelines-in-kubernetes-for-microservices-architecture-4880f744fdd6) - Architecture patterns
- [Separate CD Pipelines with GitHub Actions](https://www.codecapers.com.au/separate-cd-pipelines-for-microservices-in-a-monorepo/) - CodeCapers implementation
- [Building Monorepo CI/CD with Helm on K8s](https://www.infracloud.io/blogs/monorepo-ci-cd-helm-kubernetes/) - InfraCloud guide
- [CI/CD for .NET Microservices with Azure DevOps](https://medium.com/@venkataramanaguptha/ci-cd-for-net-microservices-with-azure-devops-end-to-end-automation-749fc127aae5) - End-to-end automation
- [Monorepo Guide: Managing Repositories & Microservices](https://www.aviator.co/blog/monorepo-a-hands-on-guide-for-managing-repositories-and-microservices/) - Aviator hands-on guide
- [Faire's CI Pipeline Transformation](https://www.linkedin.com/posts/danielepolencic_when-your-ci-pipeline-is-5000-lines-of-activity-7378756149896138752-uWwB) - LinkedIn case study
- [Implement CI/CD Strategies for Monorepos](https://graphite.dev/guides/implement-cicd-strategies-monorepos) - Graphite detailed guide
- [CI/CD for Microservices using Mono Repository](https://medium.com/@chatla-hari/ci-cd-for-microservices-architecture-using-mono-repository-ed612a2d072e) - Medium implementation
- [Best CI/CD Triggering Strategies for Monorepo](https://www.reddit.com/r/github/comments/1im6f71/best_cicd_triggering_strategies_for_a/) - Reddit GitHub community
- [Optimize CI/CD Pipeline for Microservices](https://gole.ms/blog/how-optimize-cicd-pipeline-microservices-architecture) - Optimization techniques
- [Automating Docker Builds with GitHub Actions](https://beon.tech/blog/automating-docker-builds-monorepo-github-actions/) - Beon Tech tutorial
- [Docker Monorepo CI/CD in Azure Pipelines](https://stackoverflow.com/questions/69846184/docker-microservice-monorepo-ci-cd-in-azure-pipelines) - Stack Overflow solutions
- [Monorepo Architecture and Build Pipeline](https://www.abrahamberg.com/blog/monorepo-architecture-ci-cd-and-build-pipeline/) - Architecture deep dive
- [Why Monorepo and CI/CD in Monorepo](https://informediq.com/why-monorepo-and-how-to-use-ci-cd-in-monorepo/) - Informed IQ rationale
- [CI/CD for Microservices - Microsoft Learn](https://learn.microsoft.com/en-us/azure/architecture/microservices/ci-cd) - Azure architecture guide
- [Kubernetes CI/CD Pipelines â€“ Best Practices](https://spacelift.io/blog/kubernetes-ci-cd) - Spacelift benchmarks
- [CI/CD Best Practices for Microservice Architecture](https://devtron.ai/blog/microservices-ci-cd-best-practices/) - Devtron guide
- [Build Scalable and Reliable CI/CD Pipelines With Kubernetes](https://thenewstack.io/how-to-build-scalable-and-reliable-ci-cd-pipelines-with-kubernetes/) - The New Stack
- [Monorepo CI Best Practices](https://buildkite.com/resources/blog/monorepo-ci-best-practices/) - Buildkite resources

---

### Phase 9 (Proposed): Deployment & Scaling Automation

**Objective**: Production-ready deployment with auto-scaling and CI/CD

#### Task 9.1.1: Kubernetes Auto-Scaling + CI/CD Pipeline ðŸš€ CRITICAL
**Description**: Production-ready CI/CD pipeline with path-based triggers, caching, and Kubernetes auto-scaling. Implements best practices from monorepo CI/CD optimization guide (see above section).

**Implementation**:
- **Location**: `.github/workflows/production.yml` (production pipeline)
- **K8s Manifests**: `scripts/deployment/k8s/` (deployments, services, HPA)
- **Helm Chart**: `scripts/deployment/helm/fks-platform/` (production-ready charts)
- **Scripts**: `scripts/deployment/auto_scale_k8s.sh` (HPA configuration)

**CI/CD Pipeline Components**:

1. **Path-Based Triggers** (40-60% fewer runs):
```yaml
on:
  push:
    branches: [main]
    paths:
      - 'src/services/**'
      - 'docker/**'
      - 'requirements*.txt'
```

2. **Change Detection** (selective builds):
```yaml
jobs:
  detect-changes:
    uses: dorny/paths-filter@v2
    filters: |
      api: 'src/services/api/**'
      app: 'src/services/app/**'
      ai: 'src/services/ai/**'
      data: 'src/services/data/**'
```

3. **Caching** (30-50% faster builds):
   - Docker layer caching: `cache-from: type=gha`
   - Python dependencies: `actions/cache@v4` with pip cache
   - Expected impact: 8-12 minute builds (down from 20-30)

4. **Parallel Execution** (halves duration):
   - Matrix strategy for concurrent service builds
   - Dependency-aware ordering (base images first)
   - `max-parallel: 4` to avoid resource contention

5. **Quality Gates**:
   - Ruff linting before builds
   - Pytest unit tests per service
   - Integration tests with Docker Compose services
   - Coverage reports uploaded to Codecov

6. **Staged Deployment**:
   - Stage 1: Deploy to K8s staging namespace
   - Stage 2: Smoke tests (health checks, basic API calls)
   - Stage 3: Helm upgrade to production with canary rollout
   - Stage 4: Monitor rollout status (5-minute timeout)

7. **Kubernetes Auto-Scaling**:
```yaml
# HPA configuration
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fks-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fks-app
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 4Gi
```

8. **Rollback Mechanism**:
```yaml
- name: Deploy with Health Check
  run: |
    kubectl rollout status deployment/fks-app --timeout=5m
    curl -f https://staging.fks.com/health || kubectl rollout undo deployment/fks-app
```

9. **Notifications**:
   - Slack/Discord webhook on failures
   - Success notifications for production deploys
   - Alerts for rollback triggers

**Dependencies**:
- Reference monorepo CI/CD optimization section above
- Existing: `docs/archive/completed-phases/PHASE_4_DEPLOYMENT_CHECKLIST.md`
- New: Kubernetes manifests, Helm charts, GitHub Actions workflows
- Tools: Kompose (Compose â†’ K8s conversion), Helm 3, kubectl

**Effort**: 7-10 days (increased for comprehensive CI/CD setup)

**Rationale**: 
- Research shows 20-50% build time reduction with caching + selective builds (DigitalOcean, Graphite)
- Path-based triggers reduce unnecessary runs by 40-60% (Graphite guide)
- Auto-scaling prevents manual intervention during volatile markets (Biz4Group best practices)
- Helm enables version-controlled deploys with <2 minute rollbacks

**Migration Strategy** (Gradual K8s Adoption):
- Week 1-2: Implement GitHub Actions CI/CD with path-based triggers
- Week 3-4: Convert services to K8s with Kompose, deploy to staging
- Week 5-6: Helm chart creation, production namespace setup
- Week 7-8: HPA configuration, monitoring integration, production rollout

**Acceptance Criteria**:
- [ ] GitHub Actions pipeline with path-based triggers operational
- [ ] Docker layer caching reducing builds by â‰¥30%
- [ ] Parallel service builds halving pipeline duration
- [ ] K8s HPA configured for fks_ai, fks_app, fks_data (scale 1-5 pods)
- [ ] Helm chart deployable to staging/production namespaces
- [ ] Rollback mechanism tested (auto-revert on health check failure)
- [ ] Smoke tests running post-deployment (<5 minutes)
- [ ] Slack/Discord notifications functional
- [ ] Documentation: Complete runbook in `docs/deployment/CICD_RUNBOOK.md`
- [ ] Deployment frequency: â‰¥3 deploys/day achievable

**Expected Impact**:
- Build time: 8-12 minutes (vs. 20-30 manual)
- Deployment frequency: Multiple/day (vs. weekly)
- Failure detection: <5 minutes (automated tests)
- Rollback time: <2 minutes (Helm rollback)
- Cost reduction: 40-60% fewer CI/CD minutes (selective builds)

---

#### Task 9.2.1: Multi-Repo Migration Templates (Optional) ðŸŒ³ SCALING
**Description**: Refine migration scripts for optional multi-repo split. Include CI templates for Python (pytest) and Rust (cargo test).

**Implementation**:
- Location: `scripts/migration/multi_repo_templates/`
- Templates: 
  - `python_service_template/` (FastAPI structure)
  - `rust_service_template/` (Cargo workspace)
- CI: GitHub Actions for each service (isolated testing)

**Dependencies**:
- Reference: `docs/archive/completed-phases/MULTI_REPO_IMPLEMENTATION_COMPLETE.md`
- Existing monorepo: `src/services/`

**Effort**: 3-5 days

**Rationale**: Uber's monorepo lessons suggest hybrid approach for large teams (>10 developers). Optional migration preserves current workflow while enabling future flexibility.

**Acceptance Criteria**:
- [ ] Templates functional for fks_ai, fks_app split
- [ ] CI runs independently for each service
- [ ] Documentation: Migration decision tree (when to split)
- [ ] Backward compatibility: Monorepo remains default

---

## ðŸ“‹ Task Implementation Roadmap

### Priority Matrix (Effort vs. Impact)

| Priority | Task ID | Effort | Impact | Phase | Status |
|----------|---------|--------|--------|-------|--------|
| ðŸ”¥ **P0** | 7.4.1 | 7-14d | High | 7 | Next (WFO critical for overfitting) |
| ðŸ”¥ **P0** | 5.1.1 | 3-5d | High | 5 | Retrofit (drift detection) |
| âš¡ **P1** | 6.1.1 | 5-8d | Medium | 6 | Retrofit (memory isolation) |
| âš¡ **P1** | 8.1.1 | 6-9d | High | 8 | New (CPI-Gold hedge) |
| âš¡ **P1** | 7.2.3 | 7-12d | High | 7 | New (RAG-Judge expansion) |
| ðŸ“Š **P2** | 5.2.1 | 4-7d | Medium | 5 | Retrofit (signature kernels) |
| ðŸ“Š **P2** | 6.2.1 | 3-6d | Medium | 6 | Retrofit (phase gates) |
| ðŸ“Š **P2** | 8.2.1 | 4-7d | Medium | 8 | New (Markov modules) |
| ðŸ”§ **P3** | 6.3.1 | 4-7d | Low | 6 | Retrofit (Ollama quantization) |
| ðŸ”§ **P3** | 5.3.1 | 2-4d | Low | 5 | Retrofit (on-chain schema) |
| ðŸ”§ **P3** | 7.2.4 | 16-29d | Low | 7 | Optional (detailed judge subtasks) |
| ðŸš€ **P4** | 9.1.1 | 5-8d | Medium | 9 | New (K8s auto-scale) |
| ðŸŒ³ **P5** | 9.2.1 | 3-5d | Low | 9 | New (multi-repo templates) |

**Total Effort**: 73-122 developer-days (~15-24 weeks with 1 developer, 7-12 weeks with 2)

### Implementation Sequence

**Sprint 1 (Weeks 1-2)**: Critical Stability
1. Task 5.1.1: Data drift detection (5 days)
2. Task 7.4.1: Walk-forward optimization (10 days)

**Sprint 2 (Weeks 3-4)**: Agent Optimization  
3. Task 6.1.1: Memory isolation (8 days)
4. Task 6.2.1: Phase gates (5 days)

**Sprint 3 (Weeks 5-6)**: Advanced Features
5. Task 5.2.1: Signature kernels (7 days)
6. Task 8.1.1: CPI-Gold hedge (7 days)

**Sprint 4 (Weeks 7-8)**: Strategy Expansion
7. Task 8.2.1: Markov modules (7 days)
8. Task 5.3.1: On-chain schema (3 days)

**Sprint 5 (Weeks 9-10)**: Performance & Scaling
9. Task 6.3.1: Ollama quantization (7 days)
10. Task 9.1.1: K8s auto-scaling (8 days)

**Sprint 6 (Optional)**: Multi-Repo Migration
11. Task 9.2.1: Migration templates (5 days)

---

## ðŸŽ¯ Task Allocation Guidelines

### Service-Level Assignments

| Task | Primary Service | Secondary Services | Skills Required |
|------|-----------------|-------------------|-----------------|
| 5.1.1 | fks_data | fks_main (monitoring) | Python, alibi-detect, Prometheus |
| 5.2.1 | fks_app | notebooks | Python, signatory, quant math |
| 5.3.1 | fks_data | db (PostgreSQL) | SQL, TimescaleDB, data modeling |
| 6.1.1 | fks_ai | - | LangGraph, ChromaDB, state management |
| 6.2.1 | fks_ai | fks_api | MCP standards, error handling, gates |
| 6.3.1 | fks_ai | ollama | GGUF, quantization, GPU optimization |
| 7.2.3 | fks_ai | fks_data | RAG pipelines, ChromaDB, embeddings, EODHD API |
| 7.2.4 | fks_ai | fks_app | Prompt engineering, meta-eval, debate mechanisms |
| 7.4.1 | fks_app | fks_data | Optuna, WFO, backtesting |
| 8.1.1 | fks_app | fks_data | APIs (BLS, yfinance), hedging theory |
| 8.2.1 | fks_app | notebooks | Markov chains, linear algebra, stats |
| 9.1.1 | infrastructure | all services | Kubernetes, HPA, GitHub Actions |
| 9.2.1 | infrastructure | - | Multi-repo architecture, templating |

### Effort Estimation (Developer-Days)

**Simple** (1-3 days): Configuration, minor integrations
- 5.3.1 (schema extension)

**Moderate** (4-7 days): Feature additions, medium complexity
- 5.1.1 (drift detection), 5.2.1 (kernels), 6.2.1 (gates), 6.3.1 (quantization), 8.2.1 (Markov)

**Complex** (8-14 days): Major modules, research-heavy
- 6.1.1 (memory isolation), 7.4.1 (WFO), 8.1.1 (CPI-Gold), 9.1.1 (K8s)

### Testing Requirements Per Task

**Unit Tests**: All tasks require â‰¥80% coverage
**Integration Tests**: Database/API tasks (5.x, 8.x, 9.x)
**Performance Tests**: Latency-critical tasks (6.3.1, 7.4.1)
**Validation Tests**: AI/ML tasks (5.2.1, 6.1.1, 7.4.1, 8.2.1)

---

## ðŸ“š Additional Resources & Citations

**AI Project Management**:
- [DataScience-PM: Managing AI Projects (6 Concepts)](https://www.datascience-pm.com/manage-ai-projects/)
- [Medium: How to Execute AI Projects](https://david-theil.medium.com/how-to-execute-ai-projects-94315d557569)
- [SoftKraft: Ultimate 7-Step AI Project Guide](https://www.softkraft.co/ai-project-management/)

**Algorithmic Trading Best Practices**:
- [LuxAlgo: Algo Trading Strategy Development](https://www.luxalgo.com/blog/best-practices-in-algo-trading-strategy-development/)
- [QuantStart: Best Practices for System Development](https://www.quantstart.com/articles/Best-Practices-for-Algorithmic-Trading-System-Development/)

**Platform Architecture**:
- [Biz4Group: Multi-Asset Trading Platform Guide](https://www.biz4group.com/blog/multi-asset-trading-platform-development-guide)
- [Multi-Agent Systems: CrewAI Framework Documentation](https://docs.crewai.com/)

**Academic/Technical**:
- AWS AgentScore: Agent Loop Cost Analysis
- Anthropic: Claude Judge Systems (30% Reliability Improvement)
- Stanford ML Group: Statistical Corrections in Model Evaluation
- Uber Engineering: Monorepo Lessons at Scale

**LLM-Judge & RAG Evaluation**:
- [The Rise of Agent-as-a-Judge Evaluation for LLMs](https://arxiv.org/html/2508.02994v1) - Multi-agent judge architectures
- [LLM-as-a-judge for enterprises: evaluate model alignment at scale](https://snorkel.ai/llm-as-judge-for-enterprises/) - Snorkel AI enterprise frameworks
- [Improve factual consistency with LLM Debates](https://aws.amazon.com/blogs/machine-learning/improve-factual-consistency-with-llm-debates/) - AWS ML debate mechanisms
- [LLM-as-a-Judge: Essential AI Governance for Financial Services](https://galileo.ai/blog/llm-as-a-judge-the-missing-piece-in-financial-services-ai-governance) - Financial compliance audits
- [Using Large Language Models for Automated Evaluation](https://air-governance-framework.finos.org/mitigations/mi-15_using-large-language-models-for-automated-evaluation-llm-as-a-judge-.html) - FINOS governance framework
- [LLMs-as-Judges: A Comprehensive Survey](https://arxiv.org/html/2412.05579v2) - 5-perspective categorization
- [Ensuring AI trustworthiness using LLM as a judge](https://dialonce.ai/en/blog-ai/trends/ensuring-ai-trustworthiness-llm-judge.html) - Trust mechanisms
- [Building Reliable LLM-as-a-Judge Systems](https://dzone.com/articles/llm-as-a-judge-ai-evaluation) - DZone best practices
- [Evaluating RAG with LLM-as-a-Judge](https://www.nb-data.com/p/evaluating-rag-with-llm-as-a-judge) - Ragas framework
- [LLM As a Judge: Tutorial and Best Practices](https://www.patronus.ai/llm-testing/llm-as-a-judge) - Patronus AI implementation
- [LLM-as-a-judge: a complete guide](https://www.evidentlyai.com/llm-guide/llm-as-a-judge) - Evidently AI hybrid retrieval
- [Retrieval Augmented Generation or Long-Context LLMs?](https://arxiv.org/abs/2407.16833) - RAG vs. long-context tradeoffs
- [LLM-as-a-Judge Primer](https://arize.com/llm-as-a-judge/) - Arize pre-built evaluators
- [Improving RAG Evaluation with ConsJudge](https://aclanthology.org/2024.findings-acl.1/) - ACL 2024 consistency-aware judges

**Mermaid & Dynamic Visualization**:
- [mermaid-py GitHub](https://github.com/ouhammmourachid/mermaid-py) - Python wrapper for Mermaid rendering
- [Mermaid.js Official Documentation](https://mermaid.js.org/) - Comprehensive syntax reference and examples
- [mermaid-py on PyPI](https://pypi.org/project/mermaid-py/) - Installation and API reference
- [Diagrams as Code with Mermaid](https://www.freecodecamp.org/news/diagrams-as-code-with-mermaid-github-and-vs-code/) - freeCodeCamp guide to integration
- [Auto-Updating Diagrams on Git Push](https://medium.com/@kanishks772/auto-updating-diagrams-the-magic-behind-git-push-30d1c7d80b84) - CI/CD automation patterns
- [Terramaid: IaC to Mermaid](https://github.com/RoseSecurity/Terramaid) - Infrastructure-as-code diagram generation
- [Mermaid Live Code Renderer with Python](https://medium.com/@antonio.formato/mermaid-live-code-renderer-with-python-2e8cb677e963) - Dynamic rendering techniques
- [render-md-mermaid GitHub Action](https://nielsvaneck.com/post/2021-01-31-render-md-mermaid-a-github-action/) - Automated markdown diagram rendering
- [Getting Started with "Image as Code"](https://dev.to/payel_bhattacharya_71206f/getting-started-with-image-as-code-using-mermaid-a-modern-approach-to-visualization-4llc) - Modern visualization approach
- [Automatic Diagram Generation for CI/CD](https://www.pulumi.com/blog/automating-diagramming-in-your-ci-cd/) - Pulumi automation strategies
- [Automate Code Metrics with GitHub Actions](https://devblogs.microsoft.com/dotnet/automate-code-metrics-and-class-diagrams-with-github-actions/) - Microsoft .NET class diagrams
- [Mermaid Chart: AI-Powered Diagrams](https://www.youtube.com/watch?v=WBd5pcyFeTQ) - YouTube tutorial on AI-assisted diagram generation
- [Visualizing Algorithms with Mermaid](https://www.linkedin.com/pulse/visualizing-algorithms-optimal-outcomes-mermaid-david-funyi-tumenta-l2tpe) - LinkedIn case study

---

### Best Practices Integration

**For AI Agent**:
```
Phase X, Task X.Y: "In [file_path], implement [feature]. 
Requirements: [specific_details]. 
Test with: [test_data]. 
Expected output: [acceptance_criteria]."
```

**Example Prompts**:
- Phase 8, Task 8.1: "In `tests/unit/test_ml_models.py`, add scikit-learn confusion_matrix for ASMBTR predictions. Include Bonferroni p-value adjustments. Test on BTC 2024 data."
- Phase 9, Task 9.2: "Implement CPI-Gold hedge in `strategies/hedge.py`. Use BLS CPI API and yfinance for Gold. Target Sharpe 0.48, MDD -0.51. Backtest vs. S&P 2020-2024."
- Phase 10, Task 10.1: "Build Markov module in `strategies/markov.py`. Compute steady-state for bull/bear/sideways. Validate memoryless assumption with autocorrelation tests on EUR/USD hourly data."

### Key Citations & Resources

**Academic/Technical**:
- ASMBTR Paper: Adaptive State Model on Binary Tree Representation for FX
- LangGraph Documentation: https://langchain-ai.github.io/langgraph/
- Scikit-learn Metrics: https://scikit-learn.org/stable/modules/model_evaluation.html
- CPI Data: https://www.bls.gov/cpi/
- Gold Historical: https://www.macrotrends.net/

**Best Practices**:
- LuxAlgo: https://www.luxalgo.com/blog/best-practices-in-algo-trading-strategy-development/
- QuantStart: https://www.quantstart.com/articles/Best-Practices-for-Algorithmic-Trading-System-Development/
- Biz4Group: https://www.biz4group.com/blog/ai-algo-trading-platform-development

**Internal Docs**:
- `docs/PHASE_PLAN_SUMMARY.md` - Existing phase structure
- `docs/AI_STRATEGY_INTEGRATION.md` - Original AI plan
- `docs/OPTIMIZATION_GUIDE.md` - Hyperparameter tuning
- `docs/PROJECT_HEALTH_DASHBOARD.md` - System monitoring

---

## Phase 5 Implementation Details - Data Foundation (COMPLETED Oct 30, 2025)

### Phase 5.1: EODHD API Integration âœ…
**Location**: `src/services/data/src/adapters/eodhd.py`, `src/services/data/src/collectors/fundamentals_collector.py`

**Key Components**:
- **EODHD Adapter**: Comprehensive API client with rate limiting (1000 requests/day)
- **Data Types**: Fundamentals, earnings, economic indicators, insider transactions
- **Features**: Async requests, response normalization, error handling, request building
- **Testing**: Full test suite with mocking (`src/services/data/src/tests/test_adapter_eodhd.py`)

**Usage Example**:
```python
from adapters.eodhd import EODHDAdapter
from collectors.fundamentals_collector import FundamentalsCollector

adapter = EODHDAdapter(api_key="your_key")
collector = FundamentalsCollector(adapter)

# Collect fundamentals data
fundamentals = await collector.collect_fundamentals(['AAPL', 'MSFT'])
```

### Phase 5.2: Feature Engineering Pipeline âœ…
**Location**: `src/services/app/src/features/feature_processor.py`

**Key Features**:
- **40+ Technical Indicators**: RSI, MACD, Bollinger Bands, Stochastic, Williams %R, CCI, ATR, ADX
- **Statistical Features**: Log returns, volatility (5d/21d/63d), momentum, price changes
- **Volume Features**: OBV, volume ratios, VWAP
- **Time Features**: Hour, day of week, market sessions (US/EU/Asia)
- **Microstructure**: Bid-ask spreads, price impact, volume imbalance
- **TA-Lib Integration**: With numpy fallback for maximum compatibility

**Performance**: Generates 63 features from 6 OHLCV input columns

**Usage Example**:
```python
from features.feature_processor import FeatureProcessor

processor = FeatureProcessor(min_periods=20)
features = processor.process_ohlcv_features(ohlcv_data, symbol='BTCUSDT')
# Returns DataFrame with 63 engineered features
```

### Phase 5.3: TimescaleDB Fundamentals Schema âœ…
**Location**: `sql/fundamentals_schema.sql`, `sql/migrations/003_fundamentals_core_working.sql`

**Database Tables (6 Hypertables)**:
1. **company_fundamentals**: Financial statements, ratios (PE, PB, ROE)
2. **earnings_data**: Earnings estimates vs actuals, surprise analysis
3. **economic_indicators**: Macro data (GDP, CPI, Fed rates, unemployment)
4. **insider_transactions**: Corporate insider buy/sell activity
5. **news_sentiment**: News analysis with sentiment scoring
6. **correlation_analysis**: Asset correlation tracking

**Features**: Proper TimescaleDB partitioning, compression policies, indexes for performance

**Sample Data**: US economic indicators (GDP, CPI, Fed funds rate, unemployment) pre-loaded

**Usage Example**:
```sql
-- Query economic indicators
SELECT indicator_code, value, unit FROM economic_indicators 
WHERE country = 'US' ORDER BY time DESC;

-- Query latest fundamentals
SELECT * FROM company_fundamentals 
WHERE symbol = 'AAPL' ORDER BY time DESC LIMIT 1;
```

### Phase 5 Key Files Created/Modified:
```
src/services/data/src/adapters/eodhd.py                     (355 lines) - EODHD API adapter
src/services/data/src/collectors/fundamentals_collector.py  (447 lines) - Async fundamentals collector  
src/services/app/src/features/feature_processor.py          (502 lines) - Feature engineering pipeline
src/services/app/src/cache/feature_cache.py                 (450 lines) - Redis caching layer
src/services/data/src/validators/*.py                       (1,316 lines) - Quality validators
src/services/data/src/metrics/*.py                          (670 lines) - Prometheus metrics + collector
sql/fundamentals_schema.sql                                 (450 lines) - Complete schema definition
sql/migrations/004_quality_metrics.sql                      (159 lines) - Quality metrics hypertable
monitoring/grafana/dashboards/quality_monitoring.json       (660 lines) - 8-panel dashboard
```

**Phase 5 Cumulative**: 9,227 lines added, 188/188 tests passing (Oct 30, 2025)

---

## ðŸ”„ Session Continuation Guide (Start Here on New Computer)

### Current Status (Oct 31, 2025)
**Phase 6**: Multi-Agent AI System - **100% COMPLETE** âœ…  
**Last Update**: Phase 6 deployed with Ollama llama3.2:3b (Oct 31, 2025)  
**Branch**: main  
**Test Status**: 276/276 passing (188 Phase 5 + 70 Phase 6 unit + 18 Phase 6 integration)

### What's Complete âœ…
- âœ… **Phase 6.1-6.4**: Full multi-agent system implemented (7 agents, StateGraph, ChromaDB, API)
- âœ… **Phase 6.5**: Deployment complete - fks_ai running on port 8006
- âœ… **Production Code**: 2,321 lines (agents, graph, processors, memory, API)
- âœ… **Test Code**: 2,223 lines (70 unit tests passing, 18 integration tests ready)
- âœ… **Container Config**: Dockerfile.ai fixed for Python 3.13 + absolute imports
- âœ… **Ollama Deployed**: llama3.2:3b model (2.0 GB) downloaded and operational
- âœ… **Services Running**: fks_ai (8006), ollama (11434), health checks passing
- âœ… **All code committed and pushed to GitHub**

### Quick Start Commands (Resume Work)

**1. Clone/Pull Latest** (if on new computer):
```bash
git clone https://github.com/nuniesmith/fks.git
cd fks
git pull origin main
```

**2. Start Phase 6 Services** (if not running):
```bash
# Start all dependencies
docker-compose up -d db redis web

# Start AI services with GPU support
docker-compose -f docker-compose.yml -f docker-compose.gpu.yml up -d fks_ai ollama

# Verify Ollama model is available
docker-compose exec ollama ollama list
# Expected: llama3.2:3b (2.0 GB)
```

**3. Verify Deployment**:
```bash
# Health check
curl http://localhost:8006/health
# Expected: {"status":"healthy","service":"fks_ai",...}

# Check memory system
curl http://localhost:8006/ai/agents/status | jq '.memory_status'
# Expected: {"status":"healthy","can_query":true}
```

**4. Test AI Endpoints**:
```bash
# Quick analysis test
curl -X POST http://localhost:8006/ai/analyze \
  -H "Content-Type: application/json" \
  -d '{"symbol":"BTCUSDT","timeframe":"1h"}'

# View API docs
open http://localhost:8006/docs
```

### Key Files Reference

| File | Purpose | Location |
|------|---------|----------|
| **Quick Start** | 5-min deploy guide | `PHASE_6_QUICKSTART.md` |
| **Deployment** | Full guide (392 lines) | `docs/PHASE_6_DEPLOYMENT.md` |
| **Summary** | Achievement overview | `docs/PHASE_6_COMPLETE.md` |
| **API Routes** | FastAPI endpoints | `src/services/ai/src/api/routes.py` |
| **Trading Graph** | StateGraph orchestration | `src/services/ai/src/graph/trading_graph.py` |
| **Unit Tests** | 70 tests (mocked) | `src/services/ai/tests/unit/` |
| **Integration Tests** | 18 tests (live) | `src/services/ai/tests/integration/` |

### Architecture Snapshot

**fks_ai Service**:
- **Port**: 8006
- **Container**: `docker/Dockerfile.ai` (nvidia/cuda base, Python 3.13)
- **Dependencies**: LangGraph, Ollama client, ChromaDB, sentence-transformers
- **Components**:
  - 7 Agents: Technical, Sentiment, Macro, Risk, Bull, Bear, Manager
  - StateGraph: 6-node pipeline (Analysts â†’ Debate â†’ Manager â†’ Signal â†’ Reflect â†’ Memory)
  - ChromaDB: Persistent memory at `/app/data/chroma`
  - FastAPI: 4 endpoints (/ai/analyze, /ai/debate, /ai/memory/query, /ai/agents/status)

**Service Communication**:
```
fks_app (8002) â†’ HTTP â†’ fks_ai (8006) â†’ Ollama (11434)
                                      â†“
                                  ChromaDB (memory)
```

### Expected Next Steps (30 min)

1. **Deploy** (10 min): Run docker-compose build/up commands above
2. **Verify** (5 min): Check health endpoints, confirm 7 agents operational
3. **Test** (10 min): Run integration tests, validate <5s latency
4. **Validate** (5 min): Check metrics (debate contrast >70%, signal quality)
5. **Complete** (5 min): Update docs, commit, push to GitHub

### Troubleshooting Quick Reference

**Container won't start?**
```bash
docker-compose logs fks_ai  # Check errors
docker-compose down && docker-compose -f docker-compose.yml -f docker-compose.gpu.yml up -d fks_ai ollama
```

**Ollama connection failed?**
```bash
docker-compose ps ollama  # Verify running
docker-compose exec ollama ollama list  # Check model loaded
docker-compose restart fks_ai  # Retry connection
```

**Import errors in tests?**
```bash
docker-compose exec fks_ai pip list | grep langchain  # Verify deps
docker-compose exec fks_ai python -c "from api.routes import app; print('OK')"  # Test import
```

### Session Context
- **Working Directory**: `/home/jordan/Documents/fks`
- **Git Remote**: https://github.com/nuniesmith/fks.git
- **Current Branch**: main
- **Last 3 Commits**:
  - 068e6cc: docs: Add Phase 6 complete summary
  - 741efc6: docs: Update Phase 6 completion status across all docs
  - 2cbc19d: feat: Enable fks_ai service with Phase 6 deployment config

### Success Criteria (Phase 6 â†’ 100%)
- [x] Ollama llama3.2:3b model pulled and running âœ…
- [x] Services healthy: fks_ai (8006), ollama (11434) âœ…
- [x] Health endpoint responding correctly âœ…
- [x] ChromaDB memory operational âœ…
- [x] All containers built with Python 3.13 âœ…
- [x] Import structure fixed (absolute imports) âœ…

**Phase 6 COMPLETE**: Ready to start Phase 7! ðŸŽ‰

---
*Last Updated: October 31, 2025 | Copilot Instructions v6.1*  
*Phase 6 COMPLETE (100%) - Deployed with Ollama llama3.2:3b*  
*Architecture: 8-Service Microservices | Test Status: 276/276 passing | Services: 5/8 operational*
