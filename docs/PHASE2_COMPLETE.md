# üéâ Phase 2 Complete: RAG Integration with Django

## Overview

Phase 2 focused on fully integrating the RAG (Retrieval-Augmented Generation) system with Django, creating REST API endpoints, setting up automated data ingestion with Celery, and comprehensive end-to-end testing.

## ‚úÖ What We Built

### 1. Django REST API Endpoints (views_intelligence.py)

Created **7 comprehensive API endpoints** for RAG functionality:

#### `/api/intelligence/query/` (POST)
Query the knowledge base with natural language.
```bash
curl -X POST http://localhost:8000/api/intelligence/query/ \
  -H "Content-Type: application/json" \
  -d '{"query": "What strategy works best for BTCUSDT?", "top_k": 5}'
```

**Features:**
- Natural language question answering
- Semantic search with top-k retrieval
- Source citations with similarity scores
- Redis caching (5 min TTL)
- Error handling with traceback

#### `/api/intelligence/strategy/` (POST)
Get strategy suggestions for specific symbols.
```bash
curl -X POST http://localhost:8000/api/intelligence/strategy/ \
  -H "Content-Type: application/json" \
  -d '{"symbol": "BTCUSDT", "market_condition": "trending"}'
```

**Features:**
- Symbol-specific strategy recommendations
- Market condition awareness
- Timeframe consideration
- Redis caching (10 min TTL)

#### `/api/intelligence/trades/{symbol}/` (GET)
Analyze past trading performance.
```bash
curl http://localhost:8000/api/intelligence/trades/BTCUSDT/?days=30&min_trades=5
```

**Features:**
- Historical trade analysis
- Configurable time period
- Minimum trade threshold
- Redis caching (15 min TTL)

#### `/api/intelligence/signal/` (POST)
Explain current market signals with indicators.
```bash
curl -X POST http://localhost:8000/api/intelligence/signal/ \
  -H "Content-Type: application/json" \
  -d '{"symbol": "BTCUSDT", "indicators": {"rsi": 35, "macd": -0.5}}'
```

**Features:**
- Real-time signal explanation
- Indicator interpretation
- Context-aware analysis

#### `/api/intelligence/ingest/` (POST)
Manually trigger data ingestion.
```bash
curl -X POST http://localhost:8000/api/intelligence/ingest/ \
  -H "Content-Type: application/json" \
  -d '{"type": "all", "days": 30}'
```

**Options:**
- `type`: "trades", "signals", "backtests", "all"
- `days`: Look-back period
- `limit`: Maximum documents

#### `/api/intelligence/stats/` (GET)
Get RAG system statistics.
```bash
curl http://localhost:8000/api/intelligence/stats/
```

**Returns:**
- Total documents/chunks
- Document types breakdown
- Query statistics
- Model configuration

#### `/api/intelligence/health/` (GET)
Health check endpoint.
```bash
curl http://localhost:8000/api/intelligence/health/
```

**Checks:**
- Database connectivity
- Embeddings service
- LLM availability

### 2. Automated Celery Tasks (tasks.py)

Created **5 Celery tasks** for automated RAG data management:

#### `ingest_completed_trades`
**Schedule:** Daily at 1:00 AM UTC  
**Purpose:** Ingest recently completed trades into knowledge base  
**Parameters:** `days=1` (configurable)

```python
from trading.tasks import ingest_completed_trades
result = ingest_completed_trades.delay(days=7)
```

#### `ingest_trading_signals`
**Schedule:** Daily at 1:15 AM UTC  
**Purpose:** Ingest recent trading signals  
**Parameters:** `days=1`, `limit=100`

```python
from trading.tasks import ingest_trading_signals
result = ingest_trading_signals.delay(days=7, limit=100)
```

#### `ingest_backtest_results`
**Schedule:** Weekly on Sunday at 2:00 AM UTC  
**Purpose:** Ingest recent backtest results  
**Parameters:** `limit=20`

```python
from trading.tasks import ingest_backtest_results
result = ingest_backtest_results.delay(limit=20)
```

#### `ingest_all_trading_data`
**Schedule:** Weekly on Monday at 1:00 AM UTC  
**Purpose:** Comprehensive ingestion of all trading data  
**Includes:** Trades (7 days), Signals (7 days), Backtests (20 most recent)

```python
from trading.tasks import ingest_all_trading_data
result = ingest_all_trading_data.delay()
```

**Features:**
- Multi-source ingestion
- Discord notifications
- Detailed result reporting

#### `cleanup_old_rag_data`
**Schedule:** Monthly on 1st at 4:00 AM UTC  
**Purpose:** Remove outdated documents  
**Parameters:** `days=90` (remove older than 90 days)

```python
from trading.tasks import cleanup_old_rag_data
result = cleanup_old_rag_data.delay(days=90)
```

### 3. Celery Beat Schedule Configuration

Updated `celery.py` with **5 new scheduled tasks**:

```python
# RAG ingestion schedule
'ingest-completed-trades-daily': {
    'task': 'trading.tasks.ingest_completed_trades',
    'schedule': crontab(hour=1, minute=0),  # 1:00 AM daily
    'kwargs': {'days': 1},
},
'ingest-signals-daily': {
    'task': 'trading.tasks.ingest_trading_signals',
    'schedule': crontab(hour=1, minute=15),  # 1:15 AM daily
    'kwargs': {'days': 1, 'limit': 100},
},
'ingest-backtests-weekly': {
    'task': 'trading.tasks.ingest_backtest_results',
    'schedule': crontab(hour=2, minute=0, day_of_week=0),  # Sunday 2 AM
    'kwargs': {'limit': 20},
},
'comprehensive-rag-ingestion-weekly': {
    'task': 'trading.tasks.ingest_all_trading_data',
    'schedule': crontab(hour=1, minute=0, day_of_week=1),  # Monday 1 AM
},
'cleanup-old-rag-data-monthly': {
    'task': 'trading.tasks.cleanup_old_rag_data',
    'schedule': crontab(hour=4, minute=0, day_of_month=1),  # 1st of month 4 AM
    'kwargs': {'days': 90},
},
```

### 4. URL Configuration

Updated `trading/urls.py` with **7 new API routes**:

```python
# Intelligence/RAG API endpoints
path('api/intelligence/query/', views_intelligence.query_knowledge_base),
path('api/intelligence/strategy/', views_intelligence.suggest_strategy),
path('api/intelligence/trades/<str:symbol>/', views_intelligence.analyze_trades),
path('api/intelligence/signal/', views_intelligence.explain_signal),
path('api/intelligence/ingest/', views_intelligence.ingest_data),
path('api/intelligence/stats/', views_intelligence.stats),
path('api/intelligence/health/', views_intelligence.health),
```

### 5. End-to-End Testing Script

Created `scripts/test_rag_integration.sh` - comprehensive test suite:

**Test Steps:**
1. ‚úì Check Docker containers (db, redis)
2. ‚úì Verify pgvector extension
3. ‚úì Check RAG database tables
4. ‚úì Test Python imports
5. ‚úì Check CUDA availability
6. ‚úì Test embedding generation (with timing)
7. ‚úì Test data ingestion (signals + trades)
8. ‚úì Test RAG query (if Ollama available)

**Usage:**
```bash
chmod +x scripts/test_rag_integration.sh
./scripts/test_rag_integration.sh
```

## üìä Phase 2 Metrics

### Code Added
- **views_intelligence.py**: 450+ lines (7 API endpoints)
- **tasks.py additions**: 200+ lines (5 Celery tasks)
- **celery.py updates**: 50+ lines (5 schedules)
- **urls.py updates**: 10 lines (7 routes)
- **test_rag_integration.sh**: 250+ lines (8 tests)

**Total: ~960 lines of integration code**

### API Endpoints
- **7 REST endpoints** for RAG functionality
- **4 GET** endpoints (read operations)
- **3 POST** endpoints (write/query operations)
- **Redis caching** on 3 endpoints
- **Health monitoring** included

### Celery Tasks
- **5 automated tasks** for data management
- **4 scheduled jobs** for ingestion
- **1 cleanup task** for maintenance
- **Discord notifications** for monitoring

## üöÄ Quick Start

### 1. Start Services
```bash
# Standard startup
make up

# With GPU support for RAG
make gpu-up
```

### 2. Run Tests
```bash
# End-to-end RAG test
chmod +x scripts/test_rag_integration.sh
./scripts/test_rag_integration.sh

# API health check
curl http://localhost:8000/api/intelligence/health/
```

### 3. Test API Endpoints
```bash
# Query knowledge base
curl -X POST http://localhost:8000/api/intelligence/query/ \
  -H "Content-Type: application/json" \
  -d '{"query": "What are momentum strategies?"}'

# Get stats
curl http://localhost:8000/api/intelligence/stats/

# Trigger ingestion
curl -X POST http://localhost:8000/api/intelligence/ingest/ \
  -H "Content-Type: application/json" \
  -d '{"type": "all", "days": 30}'
```

### 4. Monitor Celery Tasks
```bash
# View Celery worker logs
make logs-celery

# Access Flower UI
open http://localhost:5555

# Check scheduled tasks
docker-compose exec celery_worker celery -A fks_project inspect scheduled
```

## üìÅ Files Created/Modified

```
src/trading/
‚îú‚îÄ‚îÄ views_intelligence.py    # NEW: 7 RAG API endpoints
‚îú‚îÄ‚îÄ tasks.py                  # MODIFIED: +5 RAG tasks
‚îî‚îÄ‚îÄ urls.py                   # MODIFIED: +7 RAG routes

src/django/fks_project/
‚îî‚îÄ‚îÄ celery.py                 # MODIFIED: +5 scheduled tasks

scripts/
‚îî‚îÄ‚îÄ test_rag_integration.sh   # NEW: End-to-end testing
```

## üéØ Usage Examples

### Example 1: Query Knowledge Base
```python
import requests

response = requests.post(
    'http://localhost:8000/api/intelligence/query/',
    json={
        'query': 'What strategy works best for volatile markets?',
        'top_k': 5,
        'include_sources': True
    }
)

result = response.json()
print(f"Answer: {result['answer']}")
print(f"Sources: {result['num_sources']} documents")
```

### Example 2: Get Strategy Suggestions
```python
response = requests.post(
    'http://localhost:8000/api/intelligence/strategy/',
    json={
        'symbol': 'BTCUSDT',
        'market_condition': 'trending',
        'timeframe': '4h'
    }
)

strategy = response.json()
print(f"Strategy: {strategy['strategy']}")
```

### Example 3: Analyze Past Trades
```python
response = requests.get(
    'http://localhost:8000/api/intelligence/trades/ETHUSDT/',
    params={'days': 30, 'min_trades': 10}
)

analysis = response.json()
print(f"Analysis: {analysis['analysis']}")
```

### Example 4: Trigger Data Ingestion
```python
response = requests.post(
    'http://localhost:8000/api/intelligence/ingest/',
    json={
        'type': 'all',  # or 'trades', 'signals', 'backtests'
        'days': 7,
        'limit': 100
    }
)

result = response.json()
print(f"Ingested: {result['total_documents']} documents")
```

## üîÑ Automated Workflows

### Daily Workflow (1:00-1:15 AM UTC)
1. Ingest completed trades (last 24h)
2. Ingest trading signals (last 24h, max 100)
3. Update knowledge base indices

### Weekly Workflow (Sunday-Monday)
- **Sunday 2 AM**: Ingest backtest results (last 20)
- **Monday 1 AM**: Comprehensive ingestion (7 days of data)
- **Discord notification**: Summary of documents added

### Monthly Workflow (1st of month, 4 AM)
- Clean up old documents (> 90 days)
- Optimize database indices
- Report statistics

## üìà Performance Considerations

### Caching Strategy
- **Query endpoint**: 5 min TTL
- **Strategy endpoint**: 10 min TTL
- **Trades endpoint**: 15 min TTL
- **Stats endpoint**: No cache (always fresh)

### Task Optimization
- **Parallel processing**: Each task runs independently
- **Batch ingestion**: Process multiple documents per transaction
- **Expiry times**: Tasks expire if not executed promptly
- **Error handling**: Continue processing even if individual items fail

### Database Optimization
- **HNSW indexes**: Fast vector similarity search
- **Composite indexes**: Optimized for common queries
- **Periodic cleanup**: Remove old documents monthly
- **Connection pooling**: Efficient database connections

## üêõ Troubleshooting

### Issue: "Module 'rag' not found"
```bash
# Ensure RAG modules are in Python path
cd src
export PYTHONPATH=$PYTHONPATH:$(pwd)
```

### Issue: "pgvector extension not found"
```bash
# Enable pgvector manually
docker exec fks_db psql -U postgres -d trading_db \
  -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

### Issue: "Celery tasks not running"
```bash
# Check Celery worker status
docker-compose logs celery_worker

# Restart Celery
docker-compose restart celery_worker celery_beat
```

### Issue: "RAG health check fails"
```bash
# Check component health
curl http://localhost:8000/api/intelligence/health/

# Check database connection
docker exec fks_db pg_isready -U postgres

# Check Redis
docker exec fks_redis redis-cli ping
```

## üéì What's Next: Phase 3

Only **1 task remaining**: Create RAG UI Components

### Option A: Streamlit Integration
Add an "Intelligence" tab to the existing Streamlit app:
```python
with st.tab("Intelligence"):
    query = st.text_area("Ask about your trading:")
    if st.button("Ask"):
        result = requests.post(api_url, json={'query': query})
        st.write(result.json()['answer'])
```

### Option B: Django Template
Create a dedicated intelligence page with AJAX:
```html
<div class="intelligence-container">
    <input id="query" type="text" placeholder="Ask about trading...">
    <button onclick="queryRAG()">Ask</button>
    <div id="answer"></div>
</div>
```

### Features to Include
- Chat-style interface
- Query history
- Source citations display
- Syntax highlighting for code
- Export functionality

## üìö Documentation

- **API Reference**: See `views_intelligence.py` docstrings
- **Task Reference**: See `tasks.py` docstrings
- **Testing Guide**: Run `./scripts/test_rag_integration.sh`
- **Quick Reference**: See `QUICKREF.md`

## ‚ú® Summary

**Phase 2 Achievements:**
- ‚úÖ 7 REST API endpoints for RAG functionality
- ‚úÖ 5 Celery tasks for automated data management
- ‚úÖ Comprehensive scheduling (daily/weekly/monthly)
- ‚úÖ End-to-end testing script
- ‚úÖ Redis caching for performance
- ‚úÖ Health monitoring and stats
- ‚úÖ Discord notifications for automation

**Status**: Phase 2 Complete! Ready for UI Integration ‚úÖ

**Next**: Create user interface for querying the RAG system (Streamlit or Django template)

---

**Phase**: 2 Complete
**Next**: Phase 3 - RAG UI Components
**Updated**: 2025-10-16
